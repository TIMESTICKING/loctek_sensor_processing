{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Author Jiabao Li\n",
    "#\n",
    "# Created on Mon May 27 2024\n",
    "#\n",
    "# Copyright (c) 2024 Loctek\n",
    "#\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from preprocess import make_dataset, scale_IR, make_dataset_ex\n",
    "import torch.optim as optim\n",
    "from model import *\n",
    "from torch.utils.data import Dataset, DataLoader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, distance_dataset, IR_dataset, ground_truth, filename_dataset):\n",
    "        self.IR_dataset = IR_dataset\n",
    "        self.distance_dataset = distance_dataset\n",
    "        self.ground_truth = ground_truth\n",
    "        self.filename_dataset = filename_dataset\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.IR_dataset.shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        IR_data = self.IR_dataset[idx]\n",
    "        distance_data = self.distance_dataset[idx]\n",
    "        label = self.ground_truth[idx]\n",
    "        # 将标签转换为one-hot编码\n",
    "        label_one_hot = torch.zeros(LABEL_NUM)\n",
    "        label_one_hot[label] = 1\n",
    "        return IR_data, distance_data, label_one_hot, self.filename_dataset[idx]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "distance length is 741\n",
      "IR length is 741\n",
      "gt length is 741\n",
      "Distance train dataset: torch.Size([17547, 14, 1])\n",
      "IR train dataset: torch.Size([17547, 9, 64])\n",
      "gt train dataset: 17547\n",
      "test dataset has file amount:  torch.Size([149, 14, 1])\n"
     ]
    }
   ],
   "source": [
    "# global setting\n",
    "\n",
    "# g_modle_str = 'high'\n",
    "g_modle_str = 'low'\n",
    "# g_modle_extra_str = '3'\n",
    "g_epoch_count = 1500\n",
    "g_save_during = 0\n",
    "g_ratio_of_train = 0.8\n",
    "g_save_model_files = []\n",
    "g_lr_inited = 0.001\n",
    "\n",
    "from datetime import datetime\n",
    "lean_today = datetime.today()\n",
    "learn_model_version = \"1.1.0\"\n",
    "check_points_dir = f'checkpoints/{g_modle_str}/{lean_today.year}_{lean_today.month}_{lean_today.day}_{lean_today.hour}_{lean_today.minute}_{lean_today.second}/'\n",
    "\n",
    "# trainset, testset = make_dataset()\n",
    "trainset, testset = make_dataset_ex(g_modle_str,g_ratio_of_train)\n",
    "\n",
    "# 创建数据集和数据加载器\n",
    "train_dataset = CustomDataset(*trainset)\n",
    "test_dataset = CustomDataset(*testset)\n",
    "\n",
    "# 创建数据加载器\n",
    "# USE_BACH = BATCH\n",
    "USE_BACH = 32 # 自定义batch_size\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=USE_BACH, shuffle=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=USE_BACH, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([5.1307, 3.4815, 7.8721, 3.8005, 7.8300], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "labels = trainset[2]\n",
    "# 计算每个类别出现的次数\n",
    "num_samples = len(labels)\n",
    "num_classes = max(labels) + 1  # 假设类别标签从0开始且连续\n",
    "class_counts = [labels.count(i) for i in range(num_classes)]\n",
    "\n",
    "# 计算每个类别的权重，使用类别频率的倒数\n",
    "weights = [num_samples / class_counts[i] if class_counts[i] > 0 else 0 for i in range(num_classes)]\n",
    "\n",
    "# 转换为Tensor\n",
    "weights_tensor = torch.tensor(weights).to(mydevice)\n",
    "print(weights_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 实例化网络\n",
    "net = MyMLP().to(mydevice)\n",
    "# net = MyCNN().to(mydevice)\n",
    "\n",
    "# 定义损失函数和优化器\n",
    "criterion = nn.CrossEntropyLoss(weight=weights_tensor)\n",
    "\n",
    "optimizer = optim.Adam(net.parameters(), lr=g_lr_inited)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] loss: 7.287 f1:0.594342 acc:0.590604 pre:0.657982 recall:0.590604\n",
      "[2] loss: 6.671 f1:0.636018 acc:0.637584 pre:0.694776 recall:0.637584\n",
      "[3] loss: 6.470 f1:0.646553 acc:0.651007 pre:0.726596 recall:0.651007\n",
      "[4] loss: 6.384 f1:0.647062 acc:0.657718 pre:0.699526 recall:0.657718\n",
      "[5] loss: 6.339 f1:0.635579 acc:0.644295 pre:0.720255 recall:0.644295\n",
      "[6] loss: 6.286 f1:0.664558 acc:0.671141 pre:0.730978 recall:0.671141\n",
      "[7] loss: 6.245 f1:0.667443 acc:0.664430 pre:0.742710 recall:0.664430\n",
      "[8] loss: 6.205 f1:0.699972 acc:0.704698 pre:0.718977 recall:0.704698\n",
      "[9] loss: 6.148 f1:0.674017 acc:0.677852 pre:0.733533 recall:0.677852\n",
      "[10] loss: 6.105 f1:0.633507 acc:0.644295 pre:0.735793 recall:0.644295\n",
      "[11] loss: 6.052 f1:0.708235 acc:0.711409 pre:0.773432 recall:0.711409\n",
      "[12] loss: 5.992 f1:0.718133 acc:0.724832 pre:0.777988 recall:0.724832\n",
      "[13] loss: 5.929 f1:0.692277 acc:0.704698 pre:0.761453 recall:0.704698\n",
      "[14] loss: 5.876 f1:0.710388 acc:0.711409 pre:0.766494 recall:0.711409\n",
      "[15] loss: 5.833 f1:0.743246 acc:0.744966 pre:0.773945 recall:0.744966\n",
      "[16] loss: 5.781 f1:0.732155 acc:0.738255 pre:0.791186 recall:0.738255\n",
      "[17] loss: 5.714 f1:0.730231 acc:0.738255 pre:0.785504 recall:0.738255\n",
      "[18] loss: 5.646 f1:0.771962 acc:0.778523 pre:0.818058 recall:0.778523\n",
      "[19] loss: 5.596 f1:0.746205 acc:0.758389 pre:0.806257 recall:0.758389\n",
      "[20] loss: 5.548 f1:0.800177 acc:0.805369 pre:0.830143 recall:0.805369\n",
      "[21] loss: 5.520 f1:0.788786 acc:0.791946 pre:0.821434 recall:0.791946\n",
      "[22] loss: 5.492 f1:0.786491 acc:0.791946 pre:0.830579 recall:0.791946\n",
      "[23] loss: 5.472 f1:0.753667 acc:0.765101 pre:0.813579 recall:0.765101\n",
      "[24] loss: 5.454 f1:0.786359 acc:0.791946 pre:0.808136 recall:0.791946\n",
      "[25] loss: 5.443 f1:0.791246 acc:0.798658 pre:0.833424 recall:0.798658\n",
      "[26] loss: 5.425 f1:0.767148 acc:0.778523 pre:0.821482 recall:0.778523\n",
      "[27] loss: 5.401 f1:0.783585 acc:0.791946 pre:0.829916 recall:0.791946\n",
      "[28] loss: 5.386 f1:0.792142 acc:0.798658 pre:0.834899 recall:0.798658\n",
      "[29] loss: 5.383 f1:0.800902 acc:0.805369 pre:0.841506 recall:0.805369\n",
      "[30] loss: 5.361 f1:0.799850 acc:0.805369 pre:0.830412 recall:0.805369\n",
      "[31] loss: 5.358 f1:0.813892 acc:0.818792 pre:0.854010 recall:0.818792\n",
      "[32] loss: 5.352 f1:0.812188 acc:0.818792 pre:0.843903 recall:0.818792\n",
      "[33] loss: 5.334 f1:0.800811 acc:0.805369 pre:0.835925 recall:0.805369\n",
      "[34] loss: 5.336 f1:0.799409 acc:0.805369 pre:0.839160 recall:0.805369\n",
      "[35] loss: 5.318 f1:0.819020 acc:0.825503 pre:0.857045 recall:0.825503\n",
      "[36] loss: 5.317 f1:0.818580 acc:0.825503 pre:0.861541 recall:0.825503\n",
      "[37] loss: 5.314 f1:0.812307 acc:0.818792 pre:0.835933 recall:0.818792\n",
      "[38] loss: 5.309 f1:0.784156 acc:0.791946 pre:0.831640 recall:0.791946\n",
      "[39] loss: 5.297 f1:0.819518 acc:0.825503 pre:0.852466 recall:0.825503\n",
      "[40] loss: 5.285 f1:0.818580 acc:0.825503 pre:0.861541 recall:0.825503\n",
      "[41] loss: 5.281 f1:0.812894 acc:0.818792 pre:0.851898 recall:0.818792\n",
      "[42] loss: 5.271 f1:0.798568 acc:0.805369 pre:0.824290 recall:0.805369\n",
      "[43] loss: 5.275 f1:0.825377 acc:0.832215 pre:0.863132 recall:0.832215\n",
      "[44] loss: 5.274 f1:0.826726 acc:0.832215 pre:0.855922 recall:0.832215\n",
      "[45] loss: 5.255 f1:0.806193 acc:0.812081 pre:0.839357 recall:0.812081\n",
      "[46] loss: 5.252 f1:0.812653 acc:0.818792 pre:0.840911 recall:0.818792\n",
      "[47] loss: 5.253 f1:0.826265 acc:0.832215 pre:0.854984 recall:0.832215\n",
      "[48] loss: 5.247 f1:0.798532 acc:0.805369 pre:0.824191 recall:0.805369\n",
      "[49] loss: 5.248 f1:0.820152 acc:0.825503 pre:0.859865 recall:0.825503\n",
      "[50] loss: 5.242 f1:0.812630 acc:0.818792 pre:0.845157 recall:0.818792\n",
      "[51] loss: 5.231 f1:0.819172 acc:0.825503 pre:0.847435 recall:0.825503\n",
      "[52] loss: 5.235 f1:0.804813 acc:0.812081 pre:0.836141 recall:0.812081\n",
      "[53] loss: 5.228 f1:0.805325 acc:0.812081 pre:0.829526 recall:0.812081\n",
      "[54] loss: 5.223 f1:0.803506 acc:0.812081 pre:0.849525 recall:0.812081\n",
      "[55] loss: 5.213 f1:0.819176 acc:0.825503 pre:0.851734 recall:0.825503\n",
      "[56] loss: 5.210 f1:0.819176 acc:0.825503 pre:0.851734 recall:0.825503\n",
      "[57] loss: 5.198 f1:0.802542 acc:0.812081 pre:0.854508 recall:0.812081\n",
      "[58] loss: 5.202 f1:0.812298 acc:0.818792 pre:0.841032 recall:0.818792\n",
      "[59] loss: 5.208 f1:0.797891 acc:0.805369 pre:0.830585 recall:0.805369\n",
      "[60] loss: 5.207 f1:0.812298 acc:0.818792 pre:0.841032 recall:0.818792\n",
      "[61] loss: 5.206 f1:0.819626 acc:0.825503 pre:0.848890 recall:0.825503\n",
      "[62] loss: 5.207 f1:0.833729 acc:0.838926 pre:0.869746 recall:0.838926\n",
      "[63] loss: 5.196 f1:0.824706 acc:0.832215 pre:0.867551 recall:0.832215\n",
      "[64] loss: 5.192 f1:0.812223 acc:0.818792 pre:0.844485 recall:0.818792\n",
      "[65] loss: 5.186 f1:0.812223 acc:0.818792 pre:0.844485 recall:0.818792\n",
      "[66] loss: 5.185 f1:0.841538 acc:0.845638 pre:0.868999 recall:0.845638\n",
      "[67] loss: 5.184 f1:0.801703 acc:0.812081 pre:0.851453 recall:0.812081\n",
      "[68] loss: 5.175 f1:0.831506 acc:0.838926 pre:0.863637 recall:0.838926\n",
      "[69] loss: 5.178 f1:0.818443 acc:0.825503 pre:0.849898 recall:0.825503\n",
      "[70] loss: 5.192 f1:0.825771 acc:0.832215 pre:0.858675 recall:0.832215\n",
      "[71] loss: 5.169 f1:0.811344 acc:0.818792 pre:0.842865 recall:0.818792\n",
      "[72] loss: 5.163 f1:0.822990 acc:0.832215 pre:0.870034 recall:0.832215\n",
      "[73] loss: 5.172 f1:0.840873 acc:0.845638 pre:0.867272 recall:0.845638\n",
      "[74] loss: 5.170 f1:0.839838 acc:0.845638 pre:0.869839 recall:0.845638\n",
      "[75] loss: 5.165 f1:0.825771 acc:0.832215 pre:0.858675 recall:0.832215\n",
      "[76] loss: 5.155 f1:0.840267 acc:0.845638 pre:0.870813 recall:0.845638\n",
      "[77] loss: 5.156 f1:0.799477 acc:0.805369 pre:0.822199 recall:0.805369\n",
      "[78] loss: 5.154 f1:0.833385 acc:0.838926 pre:0.862996 recall:0.838926\n",
      "[79] loss: 5.154 f1:0.839173 acc:0.845638 pre:0.863866 recall:0.845638\n",
      "[80] loss: 5.154 f1:0.827591 acc:0.832215 pre:0.858901 recall:0.832215\n",
      "[81] loss: 5.155 f1:0.847326 acc:0.852349 pre:0.874116 recall:0.852349\n",
      "[82] loss: 5.139 f1:0.846716 acc:0.852349 pre:0.872553 recall:0.852349\n",
      "[83] loss: 5.153 f1:0.822990 acc:0.832215 pre:0.870034 recall:0.832215\n",
      "[84] loss: 5.140 f1:0.834103 acc:0.838926 pre:0.864878 recall:0.838926\n",
      "[85] loss: 5.143 f1:0.809566 acc:0.818792 pre:0.856780 recall:0.818792\n",
      "[86] loss: 5.138 f1:0.831935 acc:0.838926 pre:0.864610 recall:0.838926\n",
      "[87] loss: 5.147 f1:0.809566 acc:0.818792 pre:0.856780 recall:0.818792\n",
      "[88] loss: 5.131 f1:0.840267 acc:0.845638 pre:0.870813 recall:0.845638\n",
      "[89] loss: 5.124 f1:0.819716 acc:0.825503 pre:0.855439 recall:0.825503\n",
      "[90] loss: 5.137 f1:0.834103 acc:0.838926 pre:0.864878 recall:0.838926\n",
      "[91] loss: 5.138 f1:0.847326 acc:0.852349 pre:0.874116 recall:0.852349\n",
      "[92] loss: 5.123 f1:0.833326 acc:0.838926 pre:0.863863 recall:0.838926\n",
      "[93] loss: 5.125 f1:0.832669 acc:0.838926 pre:0.861092 recall:0.838926\n",
      "[94] loss: 5.128 f1:0.838858 acc:0.845638 pre:0.866933 recall:0.845638\n",
      "[95] loss: 5.129 f1:0.800110 acc:0.812081 pre:0.855679 recall:0.812081\n",
      "[96] loss: 5.118 f1:0.827039 acc:0.832215 pre:0.857587 recall:0.832215\n",
      "[97] loss: 5.127 f1:0.832669 acc:0.838926 pre:0.861092 recall:0.838926\n",
      "[98] loss: 5.115 f1:0.840267 acc:0.845638 pre:0.870813 recall:0.845638\n",
      "[99] loss: 5.121 f1:0.832742 acc:0.838926 pre:0.866855 recall:0.838926\n",
      "[100] loss: 5.112 f1:0.839462 acc:0.845638 pre:0.864260 recall:0.845638\n",
      "[101] loss: 5.107 f1:0.834103 acc:0.838926 pre:0.864878 recall:0.838926\n",
      "[102] loss: 5.113 f1:0.838512 acc:0.845638 pre:0.866373 recall:0.845638\n",
      "[103] loss: 5.109 f1:0.847062 acc:0.852349 pre:0.873114 recall:0.852349\n",
      "[104] loss: 5.099 f1:0.839093 acc:0.845638 pre:0.868221 recall:0.845638\n",
      "[105] loss: 5.109 f1:0.838858 acc:0.845638 pre:0.866933 recall:0.845638\n",
      "[106] loss: 5.108 f1:0.838858 acc:0.845638 pre:0.866933 recall:0.845638\n",
      "[107] loss: 5.100 f1:0.816528 acc:0.825503 pre:0.856752 recall:0.825503\n",
      "[108] loss: 5.105 f1:0.820420 acc:0.825503 pre:0.848328 recall:0.825503\n",
      "[109] loss: 5.093 f1:0.806589 acc:0.812081 pre:0.837072 recall:0.812081\n",
      "[110] loss: 5.089 f1:0.839524 acc:0.845638 pre:0.863938 recall:0.845638\n",
      "[111] loss: 5.088 f1:0.833755 acc:0.838926 pre:0.864837 recall:0.838926\n",
      "[112] loss: 5.087 f1:0.840267 acc:0.845638 pre:0.870813 recall:0.845638\n",
      "[113] loss: 5.086 f1:0.839561 acc:0.845638 pre:0.868887 recall:0.845638\n",
      "[114] loss: 5.076 f1:0.833215 acc:0.838926 pre:0.863478 recall:0.838926\n",
      "[115] loss: 5.076 f1:0.816485 acc:0.825503 pre:0.863806 recall:0.825503\n",
      "[116] loss: 5.089 f1:0.839704 acc:0.845638 pre:0.869784 recall:0.845638\n",
      "[117] loss: 5.079 f1:0.839561 acc:0.845638 pre:0.868887 recall:0.845638\n",
      "[118] loss: 5.085 f1:0.825639 acc:0.832215 pre:0.859379 recall:0.832215\n",
      "[119] loss: 5.073 f1:0.819276 acc:0.832215 pre:0.867052 recall:0.832215\n",
      "[120] loss: 5.075 f1:0.838349 acc:0.845638 pre:0.865625 recall:0.845638\n",
      "[121] loss: 5.071 f1:0.839704 acc:0.845638 pre:0.869784 recall:0.845638\n",
      "[122] loss: 5.066 f1:0.834958 acc:0.838926 pre:0.862777 recall:0.838926\n",
      "[123] loss: 5.076 f1:0.846413 acc:0.852349 pre:0.871319 recall:0.852349\n",
      "[124] loss: 5.063 f1:0.818757 acc:0.825503 pre:0.846740 recall:0.825503\n",
      "[125] loss: 5.072 f1:0.815054 acc:0.825503 pre:0.852794 recall:0.825503\n",
      "[126] loss: 5.059 f1:0.846413 acc:0.852349 pre:0.871319 recall:0.852349\n",
      "[127] loss: 5.054 f1:0.839747 acc:0.845638 pre:0.864454 recall:0.845638\n",
      "[128] loss: 5.057 f1:0.825639 acc:0.832215 pre:0.859379 recall:0.832215\n",
      "[129] loss: 5.054 f1:0.812107 acc:0.818792 pre:0.851590 recall:0.818792\n",
      "[130] loss: 5.058 f1:0.832379 acc:0.838926 pre:0.861556 recall:0.838926\n",
      "[131] loss: 5.049 f1:0.826165 acc:0.832215 pre:0.853685 recall:0.832215\n",
      "[132] loss: 5.058 f1:0.823520 acc:0.832215 pre:0.858561 recall:0.832215\n",
      "[133] loss: 5.044 f1:0.839561 acc:0.845638 pre:0.868887 recall:0.845638\n",
      "[134] loss: 5.041 f1:0.839955 acc:0.845638 pre:0.871068 recall:0.845638\n",
      "[135] loss: 5.046 f1:0.852340 acc:0.859060 pre:0.876594 recall:0.859060\n",
      "[136] loss: 5.047 f1:0.838858 acc:0.845638 pre:0.866933 recall:0.845638\n",
      "[137] loss: 5.043 f1:0.845082 acc:0.852349 pre:0.873072 recall:0.852349\n",
      "[138] loss: 5.053 f1:0.807267 acc:0.818792 pre:0.856021 recall:0.818792\n",
      "[139] loss: 5.045 f1:0.846413 acc:0.852349 pre:0.871319 recall:0.852349\n",
      "[140] loss: 5.044 f1:0.852161 acc:0.859060 pre:0.876270 recall:0.859060\n",
      "[141] loss: 5.041 f1:0.833203 acc:0.838926 pre:0.860530 recall:0.838926\n",
      "[142] loss: 5.043 f1:0.828692 acc:0.838926 pre:0.862046 recall:0.838926\n",
      "[143] loss: 5.046 f1:0.845773 acc:0.852349 pre:0.875075 recall:0.852349\n",
      "[144] loss: 5.035 f1:0.833267 acc:0.838926 pre:0.862840 recall:0.838926\n",
      "[145] loss: 5.037 f1:0.817032 acc:0.825503 pre:0.853903 recall:0.825503\n",
      "[146] loss: 5.044 f1:0.847062 acc:0.852349 pre:0.873114 recall:0.852349\n",
      "[147] loss: 5.028 f1:0.816105 acc:0.825503 pre:0.850409 recall:0.825503\n",
      "[148] loss: 5.036 f1:0.832428 acc:0.838926 pre:0.860904 recall:0.838926\n",
      "[149] loss: 5.023 f1:0.852205 acc:0.859060 pre:0.876206 recall:0.859060\n",
      "[150] loss: 5.017 f1:0.852205 acc:0.859060 pre:0.876206 recall:0.859060\n",
      "[151] loss: 5.026 f1:0.839799 acc:0.845638 pre:0.864823 recall:0.845638\n",
      "[152] loss: 5.028 f1:0.829356 acc:0.838926 pre:0.863723 recall:0.838926\n",
      "[153] loss: 5.023 f1:0.826015 acc:0.832215 pre:0.839354 recall:0.832215\n",
      "[154] loss: 5.017 f1:0.846413 acc:0.852349 pre:0.871319 recall:0.852349\n",
      "[155] loss: 5.022 f1:0.839799 acc:0.845638 pre:0.864823 recall:0.845638\n",
      "[156] loss: 5.017 f1:0.839470 acc:0.845638 pre:0.865492 recall:0.845638\n",
      "[157] loss: 5.022 f1:0.846313 acc:0.852349 pre:0.871125 recall:0.852349\n",
      "[158] loss: 5.015 f1:0.819116 acc:0.825503 pre:0.847703 recall:0.825503\n",
      "[159] loss: 5.014 f1:0.852446 acc:0.859060 pre:0.876798 recall:0.859060\n",
      "[160] loss: 5.006 f1:0.828803 acc:0.838926 pre:0.862208 recall:0.838926\n",
      "[161] loss: 5.008 f1:0.839641 acc:0.845638 pre:0.864627 recall:0.845638\n",
      "[162] loss: 5.005 f1:0.840483 acc:0.845638 pre:0.867913 recall:0.845638\n",
      "[163] loss: 4.999 f1:0.845462 acc:0.852349 pre:0.874151 recall:0.852349\n",
      "[164] loss: 5.007 f1:0.852648 acc:0.859060 pre:0.877404 recall:0.859060\n",
      "[165] loss: 5.013 f1:0.846983 acc:0.852349 pre:0.874526 recall:0.852349\n",
      "[166] loss: 5.000 f1:0.844539 acc:0.852349 pre:0.871774 recall:0.852349\n",
      "[167] loss: 5.002 f1:0.839762 acc:0.845638 pre:0.865131 recall:0.845638\n",
      "[168] loss: 5.000 f1:0.844824 acc:0.852349 pre:0.872302 recall:0.852349\n",
      "[169] loss: 4.998 f1:0.829598 acc:0.838926 pre:0.864598 recall:0.838926\n",
      "[170] loss: 4.994 f1:0.852648 acc:0.859060 pre:0.877404 recall:0.859060\n",
      "[171] loss: 5.004 f1:0.839877 acc:0.845638 pre:0.865081 recall:0.845638\n",
      "[172] loss: 4.984 f1:0.845539 acc:0.852349 pre:0.869455 recall:0.852349\n",
      "[173] loss: 4.986 f1:0.845513 acc:0.852349 pre:0.874393 recall:0.852349\n",
      "[174] loss: 4.993 f1:0.838273 acc:0.845638 pre:0.866187 recall:0.845638\n",
      "[175] loss: 4.986 f1:0.833245 acc:0.838926 pre:0.858554 recall:0.838926\n",
      "[176] loss: 4.991 f1:0.833245 acc:0.838926 pre:0.858554 recall:0.838926\n",
      "[177] loss: 4.986 f1:0.845919 acc:0.852349 pre:0.870166 recall:0.852349\n",
      "[178] loss: 4.976 f1:0.845539 acc:0.852349 pre:0.869455 recall:0.852349\n",
      "[179] loss: 4.982 f1:0.852161 acc:0.859060 pre:0.876270 recall:0.859060\n",
      "[180] loss: 4.976 f1:0.836266 acc:0.845638 pre:0.865646 recall:0.845638\n",
      "[181] loss: 4.980 f1:0.837815 acc:0.845638 pre:0.865329 recall:0.845638\n",
      "[182] loss: 4.974 f1:0.844661 acc:0.852349 pre:0.871968 recall:0.852349\n",
      "[183] loss: 4.975 f1:0.838162 acc:0.845638 pre:0.865474 recall:0.845638\n",
      "[184] loss: 4.971 f1:0.853241 acc:0.859060 pre:0.879316 recall:0.859060\n",
      "[185] loss: 4.978 f1:0.831018 acc:0.838926 pre:0.862163 recall:0.838926\n",
      "[186] loss: 4.971 f1:0.845661 acc:0.852349 pre:0.869656 recall:0.852349\n",
      "[187] loss: 4.971 f1:0.845402 acc:0.852349 pre:0.864884 recall:0.852349\n",
      "[188] loss: 4.971 f1:0.844074 acc:0.852349 pre:0.870337 recall:0.852349\n",
      "[189] loss: 4.980 f1:0.845513 acc:0.852349 pre:0.874393 recall:0.852349\n",
      "[190] loss: 4.963 f1:0.852640 acc:0.859060 pre:0.877834 recall:0.859060\n",
      "[191] loss: 4.970 f1:0.852139 acc:0.859060 pre:0.876031 recall:0.859060\n",
      "[192] loss: 4.959 f1:0.836266 acc:0.845638 pre:0.865646 recall:0.845638\n",
      "[193] loss: 4.970 f1:0.845038 acc:0.852349 pre:0.873135 recall:0.852349\n",
      "[194] loss: 4.961 f1:0.846365 acc:0.852349 pre:0.871655 recall:0.852349\n",
      "[195] loss: 4.974 f1:0.845648 acc:0.852349 pre:0.870332 recall:0.852349\n",
      "[196] loss: 4.965 f1:0.844074 acc:0.852349 pre:0.870337 recall:0.852349\n",
      "[197] loss: 4.959 f1:0.827122 acc:0.832215 pre:0.861810 recall:0.832215\n",
      "[198] loss: 4.967 f1:0.861523 acc:0.865772 pre:0.878065 recall:0.865772\n",
      "[199] loss: 4.969 f1:0.836845 acc:0.845638 pre:0.867420 recall:0.845638\n",
      "[200] loss: 4.959 f1:0.845661 acc:0.852349 pre:0.869656 recall:0.852349\n",
      "[201] loss: 4.967 f1:0.844382 acc:0.852349 pre:0.871104 recall:0.852349\n",
      "[202] loss: 4.959 f1:0.844074 acc:0.852349 pre:0.870337 recall:0.852349\n",
      "[203] loss: 4.966 f1:0.841126 acc:0.845638 pre:0.855040 recall:0.845638\n",
      "[204] loss: 4.960 f1:0.818988 acc:0.825503 pre:0.854381 recall:0.825503\n",
      "[205] loss: 4.956 f1:0.833165 acc:0.838926 pre:0.859839 recall:0.838926\n",
      "[206] loss: 4.953 f1:0.845661 acc:0.852349 pre:0.869656 recall:0.852349\n",
      "[207] loss: 4.958 f1:0.852045 acc:0.859060 pre:0.875934 recall:0.859060\n",
      "[208] loss: 4.960 f1:0.830133 acc:0.838926 pre:0.859974 recall:0.838926\n",
      "[209] loss: 4.960 f1:0.838726 acc:0.845638 pre:0.867763 recall:0.845638\n",
      "[210] loss: 4.955 f1:0.820047 acc:0.832215 pre:0.855767 recall:0.832215\n",
      "[211] loss: 4.950 f1:0.837899 acc:0.845638 pre:0.864953 recall:0.845638\n",
      "[212] loss: 4.952 f1:0.851814 acc:0.859060 pre:0.875191 recall:0.859060\n",
      "[213] loss: 4.953 f1:0.834223 acc:0.838926 pre:0.864705 recall:0.838926\n",
      "[214] loss: 4.953 f1:0.845281 acc:0.852349 pre:0.874169 recall:0.852349\n",
      "[215] loss: 4.954 f1:0.853917 acc:0.859060 pre:0.876768 recall:0.859060\n",
      "[216] loss: 4.949 f1:0.839531 acc:0.845638 pre:0.864388 recall:0.845638\n",
      "[217] loss: 4.952 f1:0.820315 acc:0.825503 pre:0.856391 recall:0.825503\n",
      "[218] loss: 4.940 f1:0.859755 acc:0.865772 pre:0.880864 recall:0.865772\n",
      "[219] loss: 4.950 f1:0.832778 acc:0.838926 pre:0.857936 recall:0.838926\n",
      "[220] loss: 4.937 f1:0.844218 acc:0.852349 pre:0.871006 recall:0.852349\n",
      "[221] loss: 4.943 f1:0.832118 acc:0.838926 pre:0.860719 recall:0.838926\n",
      "[222] loss: 4.947 f1:0.840669 acc:0.845638 pre:0.869598 recall:0.845638\n",
      "[223] loss: 4.960 f1:0.827642 acc:0.832215 pre:0.866062 recall:0.832215\n",
      "[224] loss: 4.934 f1:0.838042 acc:0.845638 pre:0.865386 recall:0.845638\n",
      "[225] loss: 4.942 f1:0.853917 acc:0.859060 pre:0.876768 recall:0.859060\n",
      "[226] loss: 4.938 f1:0.806930 acc:0.812081 pre:0.850437 recall:0.812081\n",
      "[227] loss: 4.943 f1:0.853602 acc:0.859060 pre:0.875329 recall:0.859060\n",
      "[228] loss: 4.930 f1:0.838726 acc:0.845638 pre:0.867763 recall:0.845638\n",
      "[229] loss: 4.930 f1:0.839486 acc:0.845638 pre:0.864272 recall:0.845638\n",
      "[230] loss: 4.942 f1:0.854728 acc:0.859060 pre:0.871825 recall:0.859060\n",
      "[231] loss: 4.930 f1:0.838631 acc:0.845638 pre:0.867008 recall:0.845638\n",
      "[232] loss: 4.933 f1:0.847186 acc:0.852349 pre:0.869067 recall:0.852349\n",
      "[233] loss: 4.934 f1:0.841016 acc:0.845638 pre:0.870677 recall:0.845638\n",
      "[234] loss: 4.927 f1:0.860331 acc:0.865772 pre:0.883590 recall:0.865772\n",
      "[235] loss: 4.948 f1:0.846418 acc:0.852349 pre:0.872292 recall:0.852349\n",
      "[236] loss: 4.931 f1:0.847709 acc:0.852349 pre:0.871959 recall:0.852349\n",
      "[237] loss: 4.933 f1:0.847446 acc:0.852349 pre:0.871129 recall:0.852349\n",
      "[238] loss: 4.924 f1:0.848366 acc:0.852349 pre:0.865087 recall:0.852349\n",
      "[239] loss: 4.941 f1:0.845996 acc:0.852349 pre:0.870424 recall:0.852349\n",
      "[240] loss: 4.927 f1:0.839531 acc:0.845638 pre:0.864388 recall:0.845638\n",
      "[241] loss: 4.933 f1:0.815002 acc:0.818792 pre:0.855958 recall:0.818792\n",
      "[242] loss: 4.915 f1:0.843175 acc:0.845638 pre:0.860004 recall:0.845638\n",
      "[243] loss: 4.925 f1:0.841194 acc:0.845638 pre:0.866624 recall:0.845638\n",
      "[244] loss: 4.926 f1:0.861029 acc:0.865772 pre:0.876723 recall:0.865772\n",
      "[245] loss: 4.914 f1:0.836590 acc:0.838926 pre:0.859932 recall:0.838926\n",
      "[246] loss: 4.929 f1:0.837815 acc:0.845638 pre:0.865329 recall:0.845638\n",
      "[247] loss: 4.929 f1:0.835958 acc:0.838926 pre:0.866972 recall:0.838926\n",
      "[248] loss: 4.925 f1:0.861160 acc:0.865772 pre:0.880600 recall:0.865772\n",
      "[249] loss: 4.914 f1:0.839810 acc:0.845638 pre:0.865249 recall:0.845638\n",
      "[250] loss: 4.919 f1:0.841458 acc:0.845638 pre:0.867455 recall:0.845638\n",
      "[251] loss: 4.915 f1:0.848776 acc:0.852349 pre:0.871581 recall:0.852349\n",
      "[252] loss: 4.926 f1:0.847520 acc:0.852349 pre:0.870178 recall:0.852349\n",
      "[253] loss: 4.915 f1:0.848468 acc:0.852349 pre:0.865045 recall:0.852349\n",
      "[254] loss: 4.921 f1:0.835664 acc:0.838926 pre:0.866041 recall:0.838926\n",
      "[255] loss: 4.917 f1:0.831755 acc:0.838926 pre:0.859346 recall:0.838926\n",
      "[256] loss: 4.911 f1:0.842211 acc:0.845638 pre:0.865853 recall:0.845638\n",
      "[257] loss: 4.909 f1:0.849383 acc:0.852349 pre:0.874424 recall:0.852349\n",
      "[258] loss: 4.910 f1:0.839717 acc:0.845638 pre:0.865429 recall:0.845638\n",
      "[259] loss: 4.902 f1:0.835526 acc:0.845638 pre:0.875899 recall:0.845638\n",
      "[260] loss: 4.915 f1:0.826016 acc:0.832215 pre:0.857480 recall:0.832215\n",
      "[261] loss: 4.908 f1:0.842073 acc:0.845638 pre:0.866176 recall:0.845638\n",
      "[262] loss: 4.900 f1:0.836342 acc:0.838926 pre:0.865974 recall:0.838926\n",
      "[263] loss: 4.912 f1:0.848429 acc:0.852349 pre:0.870502 recall:0.852349\n",
      "[264] loss: 4.900 f1:0.840819 acc:0.845638 pre:0.865847 recall:0.845638\n",
      "[265] loss: 4.897 f1:0.843987 acc:0.845638 pre:0.855115 recall:0.845638\n",
      "[266] loss: 4.912 f1:0.844532 acc:0.845638 pre:0.854121 recall:0.845638\n",
      "[267] loss: 4.899 f1:0.825965 acc:0.832215 pre:0.869935 recall:0.832215\n",
      "[268] loss: 4.896 f1:0.856991 acc:0.859060 pre:0.866884 recall:0.859060\n",
      "[269] loss: 4.901 f1:0.832547 acc:0.838926 pre:0.869352 recall:0.838926\n",
      "[270] loss: 4.910 f1:0.849862 acc:0.852349 pre:0.873458 recall:0.852349\n",
      "[271] loss: 4.902 f1:0.843232 acc:0.845638 pre:0.864994 recall:0.845638\n",
      "[272] loss: 4.890 f1:0.817505 acc:0.825503 pre:0.863259 recall:0.825503\n",
      "[273] loss: 4.899 f1:0.848513 acc:0.852349 pre:0.870751 recall:0.852349\n",
      "[274] loss: 4.900 f1:0.848429 acc:0.852349 pre:0.870502 recall:0.852349\n",
      "[275] loss: 4.887 f1:0.841579 acc:0.845638 pre:0.870845 recall:0.845638\n",
      "[276] loss: 4.887 f1:0.858238 acc:0.859060 pre:0.872534 recall:0.859060\n",
      "[277] loss: 4.911 f1:0.831991 acc:0.832215 pre:0.840019 recall:0.832215\n",
      "[278] loss: 4.900 f1:0.863607 acc:0.865772 pre:0.876755 recall:0.865772\n",
      "[279] loss: 4.897 f1:0.855847 acc:0.859060 pre:0.875225 recall:0.859060\n",
      "[280] loss: 4.883 f1:0.844557 acc:0.845638 pre:0.861138 recall:0.845638\n",
      "[281] loss: 4.889 f1:0.826385 acc:0.832215 pre:0.863151 recall:0.832215\n",
      "[282] loss: 4.885 f1:0.852186 acc:0.852349 pre:0.867341 recall:0.852349\n",
      "[283] loss: 4.880 f1:0.855763 acc:0.859060 pre:0.874977 recall:0.859060\n",
      "[284] loss: 4.883 f1:0.857696 acc:0.859060 pre:0.877539 recall:0.859060\n",
      "[285] loss: 4.889 f1:0.832204 acc:0.838926 pre:0.867395 recall:0.838926\n",
      "[286] loss: 4.882 f1:0.862227 acc:0.865772 pre:0.880702 recall:0.865772\n",
      "[287] loss: 4.889 f1:0.863607 acc:0.865772 pre:0.876755 recall:0.865772\n",
      "[288] loss: 4.868 f1:0.856654 acc:0.859060 pre:0.869052 recall:0.859060\n",
      "[289] loss: 4.876 f1:0.843424 acc:0.845638 pre:0.854880 recall:0.845638\n",
      "[290] loss: 4.866 f1:0.849938 acc:0.852349 pre:0.863836 recall:0.852349\n",
      "[291] loss: 4.867 f1:0.842770 acc:0.845638 pre:0.864652 recall:0.845638\n",
      "[292] loss: 4.873 f1:0.850495 acc:0.852349 pre:0.862086 recall:0.852349\n",
      "[293] loss: 4.872 f1:0.856423 acc:0.859060 pre:0.873109 recall:0.859060\n",
      "[294] loss: 4.876 f1:0.856042 acc:0.859060 pre:0.883167 recall:0.859060\n",
      "[295] loss: 4.865 f1:0.870199 acc:0.872483 pre:0.892439 recall:0.872483\n",
      "[296] loss: 4.867 f1:0.849073 acc:0.852349 pre:0.873301 recall:0.852349\n",
      "[297] loss: 4.857 f1:0.830592 acc:0.832215 pre:0.858903 recall:0.832215\n",
      "[298] loss: 4.858 f1:0.857247 acc:0.859060 pre:0.883766 recall:0.859060\n",
      "[299] loss: 4.866 f1:0.842412 acc:0.845638 pre:0.865158 recall:0.845638\n",
      "[300] loss: 4.861 f1:0.863233 acc:0.865772 pre:0.876909 recall:0.865772\n",
      "[301] loss: 4.864 f1:0.857591 acc:0.859060 pre:0.877535 recall:0.859060\n",
      "[302] loss: 4.852 f1:0.842520 acc:0.845638 pre:0.865179 recall:0.845638\n",
      "[303] loss: 4.869 f1:0.849672 acc:0.852349 pre:0.860873 recall:0.852349\n",
      "[304] loss: 4.851 f1:0.839804 acc:0.845638 pre:0.871343 recall:0.845638\n",
      "[305] loss: 4.865 f1:0.844520 acc:0.845638 pre:0.871379 recall:0.845638\n",
      "[306] loss: 4.858 f1:0.832204 acc:0.838926 pre:0.867395 recall:0.838926\n",
      "[307] loss: 4.860 f1:0.857247 acc:0.859060 pre:0.883766 recall:0.859060\n",
      "[308] loss: 4.862 f1:0.850886 acc:0.852349 pre:0.866898 recall:0.852349\n",
      "[309] loss: 4.861 f1:0.864154 acc:0.865772 pre:0.887099 recall:0.865772\n",
      "[310] loss: 4.850 f1:0.851046 acc:0.852349 pre:0.866390 recall:0.852349\n",
      "[311] loss: 4.857 f1:0.850651 acc:0.852349 pre:0.860769 recall:0.852349\n",
      "[312] loss: 4.864 f1:0.835979 acc:0.838926 pre:0.858159 recall:0.838926\n",
      "[313] loss: 4.855 f1:0.857215 acc:0.859060 pre:0.865561 recall:0.859060\n",
      "[314] loss: 4.846 f1:0.850331 acc:0.852349 pre:0.872131 recall:0.852349\n",
      "[315] loss: 4.850 f1:0.870178 acc:0.872483 pre:0.886338 recall:0.872483\n",
      "[316] loss: 4.844 f1:0.830774 acc:0.832215 pre:0.852609 recall:0.832215\n",
      "[317] loss: 4.844 f1:0.850187 acc:0.852349 pre:0.871698 recall:0.852349\n",
      "[318] loss: 4.846 f1:0.856835 acc:0.859060 pre:0.868057 recall:0.859060\n",
      "[319] loss: 4.853 f1:0.857217 acc:0.859060 pre:0.878383 recall:0.859060\n",
      "[320] loss: 4.847 f1:0.871504 acc:0.872483 pre:0.877482 recall:0.872483\n",
      "[321] loss: 4.846 f1:0.844557 acc:0.845638 pre:0.861138 recall:0.845638\n",
      "[322] loss: 4.858 f1:0.838045 acc:0.838926 pre:0.854279 recall:0.838926\n",
      "[323] loss: 4.856 f1:0.826798 acc:0.825503 pre:0.851960 recall:0.825503\n",
      "[324] loss: 4.851 f1:0.844108 acc:0.845638 pre:0.859065 recall:0.845638\n",
      "[325] loss: 4.833 f1:0.849442 acc:0.852349 pre:0.877992 recall:0.852349\n",
      "[326] loss: 4.832 f1:0.855779 acc:0.859060 pre:0.878357 recall:0.859060\n",
      "[327] loss: 4.843 f1:0.849442 acc:0.852349 pre:0.877992 recall:0.852349\n",
      "[328] loss: 4.847 f1:0.849938 acc:0.852349 pre:0.863836 recall:0.852349\n",
      "[329] loss: 4.835 f1:0.840625 acc:0.845638 pre:0.871442 recall:0.845638\n",
      "[330] loss: 4.837 f1:0.856654 acc:0.859060 pre:0.869052 recall:0.859060\n",
      "[331] loss: 4.846 f1:0.825876 acc:0.832215 pre:0.859424 recall:0.832215\n",
      "[332] loss: 4.834 f1:0.850597 acc:0.852349 pre:0.868195 recall:0.852349\n",
      "[333] loss: 4.833 f1:0.856510 acc:0.859060 pre:0.869240 recall:0.859060\n",
      "[334] loss: 4.833 f1:0.850006 acc:0.852349 pre:0.864012 recall:0.852349\n",
      "[335] loss: 4.836 f1:0.839612 acc:0.845638 pre:0.870679 recall:0.845638\n",
      "[336] loss: 4.835 f1:0.850639 acc:0.852349 pre:0.876722 recall:0.852349\n",
      "[337] loss: 4.839 f1:0.850639 acc:0.852349 pre:0.876722 recall:0.852349\n",
      "[338] loss: 4.841 f1:0.841788 acc:0.845638 pre:0.872762 recall:0.845638\n",
      "[339] loss: 4.840 f1:0.845011 acc:0.845638 pre:0.851685 recall:0.845638\n",
      "[340] loss: 4.838 f1:0.842259 acc:0.845638 pre:0.867688 recall:0.845638\n",
      "[341] loss: 4.835 f1:0.810300 acc:0.818792 pre:0.856093 recall:0.818792\n",
      "[342] loss: 4.830 f1:0.843822 acc:0.845638 pre:0.858505 recall:0.845638\n",
      "[343] loss: 4.831 f1:0.834636 acc:0.838926 pre:0.861545 recall:0.838926\n",
      "[344] loss: 4.829 f1:0.856594 acc:0.859060 pre:0.881060 recall:0.859060\n",
      "[345] loss: 4.827 f1:0.830383 acc:0.832215 pre:0.858866 recall:0.832215\n",
      "[346] loss: 4.835 f1:0.835454 acc:0.838926 pre:0.855317 recall:0.838926\n",
      "[347] loss: 4.825 f1:0.845055 acc:0.845638 pre:0.852380 recall:0.845638\n",
      "[348] loss: 4.827 f1:0.852605 acc:0.859060 pre:0.882152 recall:0.859060\n",
      "[349] loss: 4.832 f1:0.820622 acc:0.825503 pre:0.853680 recall:0.825503\n",
      "[350] loss: 4.842 f1:0.843529 acc:0.845638 pre:0.853648 recall:0.845638\n",
      "[351] loss: 4.838 f1:0.851749 acc:0.852349 pre:0.857926 recall:0.852349\n",
      "[352] loss: 4.826 f1:0.855330 acc:0.859060 pre:0.876098 recall:0.859060\n",
      "[353] loss: 4.822 f1:0.842566 acc:0.845638 pre:0.856950 recall:0.845638\n",
      "[354] loss: 4.838 f1:0.840602 acc:0.845638 pre:0.866676 recall:0.845638\n",
      "[355] loss: 4.822 f1:0.837960 acc:0.838926 pre:0.849362 recall:0.838926\n",
      "[356] loss: 4.833 f1:0.824010 acc:0.825503 pre:0.848680 recall:0.825503\n",
      "[357] loss: 4.831 f1:0.836783 acc:0.838926 pre:0.864077 recall:0.838926\n",
      "[358] loss: 4.821 f1:0.835896 acc:0.838926 pre:0.867483 recall:0.838926\n",
      "[359] loss: 4.820 f1:0.836678 acc:0.838926 pre:0.864075 recall:0.838926\n",
      "[360] loss: 4.824 f1:0.819699 acc:0.818792 pre:0.839941 recall:0.818792\n",
      "[361] loss: 4.824 f1:0.831703 acc:0.838926 pre:0.865188 recall:0.838926\n",
      "[362] loss: 4.827 f1:0.834342 acc:0.838926 pre:0.860529 recall:0.838926\n",
      "[363] loss: 4.827 f1:0.835990 acc:0.838926 pre:0.864870 recall:0.838926\n",
      "[364] loss: 4.832 f1:0.824164 acc:0.832215 pre:0.861904 recall:0.832215\n",
      "[365] loss: 4.822 f1:0.841758 acc:0.845638 pre:0.872253 recall:0.845638\n",
      "[366] loss: 4.816 f1:0.850911 acc:0.852349 pre:0.861232 recall:0.852349\n",
      "[367] loss: 4.827 f1:0.835225 acc:0.838926 pre:0.860934 recall:0.838926\n",
      "[368] loss: 4.817 f1:0.836640 acc:0.838926 pre:0.861975 recall:0.838926\n",
      "[369] loss: 4.816 f1:0.850926 acc:0.852349 pre:0.863588 recall:0.852349\n",
      "[370] loss: 4.812 f1:0.834576 acc:0.838926 pre:0.861215 recall:0.838926\n",
      "[371] loss: 4.833 f1:0.855550 acc:0.859060 pre:0.880514 recall:0.859060\n",
      "[372] loss: 4.824 f1:0.833249 acc:0.838926 pre:0.859980 recall:0.838926\n",
      "[373] loss: 4.818 f1:0.865424 acc:0.865772 pre:0.876970 recall:0.865772\n",
      "[374] loss: 4.824 f1:0.838665 acc:0.838926 pre:0.849854 recall:0.838926\n",
      "[375] loss: 4.831 f1:0.843452 acc:0.845638 pre:0.855156 recall:0.845638\n",
      "[376] loss: 4.824 f1:0.826987 acc:0.832215 pre:0.852243 recall:0.832215\n",
      "[377] loss: 4.823 f1:0.834636 acc:0.838926 pre:0.861545 recall:0.838926\n",
      "[378] loss: 4.818 f1:0.839612 acc:0.845638 pre:0.870679 recall:0.845638\n",
      "[379] loss: 4.813 f1:0.829518 acc:0.832215 pre:0.861457 recall:0.832215\n",
      "[380] loss: 4.819 f1:0.841406 acc:0.845638 pre:0.865520 recall:0.845638\n",
      "[381] loss: 4.824 f1:0.829749 acc:0.832215 pre:0.850906 recall:0.832215\n",
      "[382] loss: 4.828 f1:0.850482 acc:0.852349 pre:0.860900 recall:0.852349\n",
      "[383] loss: 4.813 f1:0.841496 acc:0.845638 pre:0.871315 recall:0.845638\n",
      "[384] loss: 4.810 f1:0.842937 acc:0.845638 pre:0.860695 recall:0.845638\n",
      "[385] loss: 4.823 f1:0.843923 acc:0.845638 pre:0.850429 recall:0.845638\n",
      "[386] loss: 4.811 f1:0.827977 acc:0.832215 pre:0.861443 recall:0.832215\n",
      "[387] loss: 4.807 f1:0.835454 acc:0.838926 pre:0.855317 recall:0.838926\n",
      "[388] loss: 4.809 f1:0.851242 acc:0.852349 pre:0.860063 recall:0.852349\n",
      "[389] loss: 4.812 f1:0.842983 acc:0.845638 pre:0.854161 recall:0.845638\n",
      "[390] loss: 4.805 f1:0.829963 acc:0.832215 pre:0.844812 recall:0.832215\n",
      "[391] loss: 4.803 f1:0.841654 acc:0.845638 pre:0.861567 recall:0.845638\n",
      "[392] loss: 4.809 f1:0.856547 acc:0.859060 pre:0.875068 recall:0.859060\n",
      "[393] loss: 4.802 f1:0.826501 acc:0.832215 pre:0.853992 recall:0.832215\n",
      "[394] loss: 4.813 f1:0.835363 acc:0.838926 pre:0.865643 recall:0.838926\n",
      "[395] loss: 4.797 f1:0.821615 acc:0.825503 pre:0.857112 recall:0.825503\n",
      "[396] loss: 4.809 f1:0.845576 acc:0.845638 pre:0.857853 recall:0.845638\n",
      "[397] loss: 4.805 f1:0.850018 acc:0.852349 pre:0.859868 recall:0.852349\n",
      "[398] loss: 4.807 f1:0.818635 acc:0.818792 pre:0.838000 recall:0.818792\n",
      "[399] loss: 4.803 f1:0.848951 acc:0.852349 pre:0.862900 recall:0.852349\n",
      "[400] loss: 4.819 f1:0.828179 acc:0.832215 pre:0.850813 recall:0.832215\n",
      "[401] loss: 4.804 f1:0.850011 acc:0.852349 pre:0.865192 recall:0.852349\n",
      "[402] loss: 4.796 f1:0.831176 acc:0.832215 pre:0.842121 recall:0.832215\n",
      "[403] loss: 4.805 f1:0.856739 acc:0.859060 pre:0.866822 recall:0.859060\n",
      "[404] loss: 4.803 f1:0.828775 acc:0.832215 pre:0.856776 recall:0.832215\n",
      "[405] loss: 4.805 f1:0.850200 acc:0.852349 pre:0.858115 recall:0.852349\n",
      "[406] loss: 4.819 f1:0.844546 acc:0.845638 pre:0.859651 recall:0.845638\n",
      "[407] loss: 4.789 f1:0.842749 acc:0.845638 pre:0.860186 recall:0.845638\n",
      "[408] loss: 4.801 f1:0.869489 acc:0.872483 pre:0.882204 recall:0.872483\n",
      "[409] loss: 4.802 f1:0.849145 acc:0.852349 pre:0.869713 recall:0.852349\n",
      "[410] loss: 4.791 f1:0.820476 acc:0.825503 pre:0.848889 recall:0.825503\n",
      "[411] loss: 4.803 f1:0.855674 acc:0.859060 pre:0.871358 recall:0.859060\n",
      "[412] loss: 4.802 f1:0.831283 acc:0.832215 pre:0.843261 recall:0.832215\n",
      "[413] loss: 4.790 f1:0.850903 acc:0.852349 pre:0.862151 recall:0.852349\n",
      "[414] loss: 4.797 f1:0.856739 acc:0.859060 pre:0.866822 recall:0.859060\n",
      "[415] loss: 4.792 f1:0.849686 acc:0.852349 pre:0.863233 recall:0.852349\n",
      "[416] loss: 4.793 f1:0.798235 acc:0.798658 pre:0.812133 recall:0.798658\n",
      "[417] loss: 4.794 f1:0.844746 acc:0.845638 pre:0.862542 recall:0.845638\n",
      "[418] loss: 4.799 f1:0.834985 acc:0.838926 pre:0.852925 recall:0.838926\n",
      "[419] loss: 4.785 f1:0.842516 acc:0.845638 pre:0.865124 recall:0.845638\n",
      "[420] loss: 4.785 f1:0.833950 acc:0.838926 pre:0.865933 recall:0.838926\n",
      "[421] loss: 4.795 f1:0.848601 acc:0.852349 pre:0.864163 recall:0.852349\n",
      "[422] loss: 4.795 f1:0.844698 acc:0.845638 pre:0.856759 recall:0.845638\n",
      "[423] loss: 4.789 f1:0.849686 acc:0.852349 pre:0.863233 recall:0.852349\n",
      "[424] loss: 4.787 f1:0.858448 acc:0.859060 pre:0.867260 recall:0.859060\n",
      "[425] loss: 4.798 f1:0.850947 acc:0.852349 pre:0.864571 recall:0.852349\n",
      "[426] loss: 4.788 f1:0.842432 acc:0.845638 pre:0.858218 recall:0.845638\n",
      "[427] loss: 4.792 f1:0.858270 acc:0.859060 pre:0.865935 recall:0.859060\n",
      "[428] loss: 4.785 f1:0.857069 acc:0.859060 pre:0.869955 recall:0.859060\n",
      "[429] loss: 4.782 f1:0.863268 acc:0.865772 pre:0.875881 recall:0.865772\n",
      "[430] loss: 4.777 f1:0.857091 acc:0.859060 pre:0.870006 recall:0.859060\n",
      "[431] loss: 4.774 f1:0.849670 acc:0.852349 pre:0.859047 recall:0.852349\n",
      "[432] loss: 4.787 f1:0.848589 acc:0.852349 pre:0.873362 recall:0.852349\n",
      "[433] loss: 4.784 f1:0.856544 acc:0.859060 pre:0.865235 recall:0.859060\n",
      "[434] loss: 4.780 f1:0.856333 acc:0.859060 pre:0.868992 recall:0.859060\n",
      "[435] loss: 4.782 f1:0.829147 acc:0.832215 pre:0.848249 recall:0.832215\n",
      "[436] loss: 4.776 f1:0.844457 acc:0.845638 pre:0.857070 recall:0.845638\n",
      "[437] loss: 4.779 f1:0.870251 acc:0.872483 pre:0.880641 recall:0.872483\n",
      "[438] loss: 4.783 f1:0.857600 acc:0.859060 pre:0.866578 recall:0.859060\n",
      "[439] loss: 4.768 f1:0.823875 acc:0.825503 pre:0.834155 recall:0.825503\n",
      "[440] loss: 4.776 f1:0.830139 acc:0.832215 pre:0.846452 recall:0.832215\n",
      "[441] loss: 4.769 f1:0.850597 acc:0.852349 pre:0.859316 recall:0.852349\n",
      "[442] loss: 4.780 f1:0.850919 acc:0.852349 pre:0.857827 recall:0.852349\n",
      "[443] loss: 4.773 f1:0.856896 acc:0.859060 pre:0.868420 recall:0.859060\n",
      "[444] loss: 4.774 f1:0.868824 acc:0.872483 pre:0.886635 recall:0.872483\n",
      "[445] loss: 4.777 f1:0.850887 acc:0.852349 pre:0.869540 recall:0.852349\n",
      "[446] loss: 4.774 f1:0.855013 acc:0.859060 pre:0.872924 recall:0.859060\n",
      "[447] loss: 4.766 f1:0.857566 acc:0.859060 pre:0.866331 recall:0.859060\n",
      "[448] loss: 4.787 f1:0.843501 acc:0.845638 pre:0.857039 recall:0.845638\n",
      "[449] loss: 4.765 f1:0.849664 acc:0.852349 pre:0.863182 recall:0.852349\n",
      "[450] loss: 4.765 f1:0.849686 acc:0.852349 pre:0.863233 recall:0.852349\n",
      "[451] loss: 4.770 f1:0.850978 acc:0.852349 pre:0.864542 recall:0.852349\n",
      "[452] loss: 4.770 f1:0.849686 acc:0.852349 pre:0.863233 recall:0.852349\n",
      "[453] loss: 4.774 f1:0.849842 acc:0.852349 pre:0.861194 recall:0.852349\n",
      "[454] loss: 4.770 f1:0.842432 acc:0.845638 pre:0.858218 recall:0.845638\n",
      "[455] loss: 4.775 f1:0.849664 acc:0.852349 pre:0.863182 recall:0.852349\n",
      "[456] loss: 4.768 f1:0.862886 acc:0.865772 pre:0.871566 recall:0.865772\n",
      "[457] loss: 4.775 f1:0.870385 acc:0.872483 pre:0.880966 recall:0.872483\n",
      "[458] loss: 4.775 f1:0.849664 acc:0.852349 pre:0.863182 recall:0.852349\n",
      "[459] loss: 4.767 f1:0.850302 acc:0.852349 pre:0.856614 recall:0.852349\n",
      "[460] loss: 4.777 f1:0.841154 acc:0.845638 pre:0.858871 recall:0.845638\n",
      "[461] loss: 4.774 f1:0.842728 acc:0.845638 pre:0.856956 recall:0.845638\n",
      "[462] loss: 4.763 f1:0.863250 acc:0.865772 pre:0.878600 recall:0.865772\n",
      "[463] loss: 4.774 f1:0.848957 acc:0.852349 pre:0.863059 recall:0.852349\n",
      "[464] loss: 4.760 f1:0.851229 acc:0.852349 pre:0.867773 recall:0.852349\n",
      "[465] loss: 4.764 f1:0.836917 acc:0.838926 pre:0.840429 recall:0.838926\n",
      "[466] loss: 4.773 f1:0.855849 acc:0.859060 pre:0.868979 recall:0.859060\n",
      "[467] loss: 4.765 f1:0.829682 acc:0.832215 pre:0.843045 recall:0.832215\n",
      "[468] loss: 4.767 f1:0.844286 acc:0.845638 pre:0.855387 recall:0.845638\n",
      "[469] loss: 4.763 f1:0.837434 acc:0.838926 pre:0.845440 recall:0.838926\n",
      "[470] loss: 4.762 f1:0.844634 acc:0.845638 pre:0.861284 recall:0.845638\n",
      "[471] loss: 4.768 f1:0.825721 acc:0.825503 pre:0.832042 recall:0.825503\n",
      "[472] loss: 4.760 f1:0.849686 acc:0.852349 pre:0.863233 recall:0.852349\n",
      "[473] loss: 4.773 f1:0.857303 acc:0.859060 pre:0.863962 recall:0.859060\n",
      "[474] loss: 4.761 f1:0.842458 acc:0.845638 pre:0.854952 recall:0.845638\n",
      "[475] loss: 4.757 f1:0.862977 acc:0.865772 pre:0.875245 recall:0.865772\n",
      "[476] loss: 4.751 f1:0.850482 acc:0.852349 pre:0.860900 recall:0.852349\n",
      "[477] loss: 4.760 f1:0.805885 acc:0.805369 pre:0.840788 recall:0.805369\n",
      "[478] loss: 4.750 f1:0.849686 acc:0.852349 pre:0.863233 recall:0.852349\n",
      "[479] loss: 4.758 f1:0.836755 acc:0.838926 pre:0.846673 recall:0.838926\n",
      "[480] loss: 4.744 f1:0.844071 acc:0.845638 pre:0.853963 recall:0.845638\n",
      "[481] loss: 4.751 f1:0.850311 acc:0.852349 pre:0.859640 recall:0.852349\n",
      "[482] loss: 4.757 f1:0.850029 acc:0.852349 pre:0.869736 recall:0.852349\n",
      "[483] loss: 4.754 f1:0.842964 acc:0.845638 pre:0.857774 recall:0.845638\n",
      "[484] loss: 4.775 f1:0.830855 acc:0.832215 pre:0.833558 recall:0.832215\n",
      "[485] loss: 4.756 f1:0.836457 acc:0.838926 pre:0.844990 recall:0.838926\n",
      "[486] loss: 4.768 f1:0.828937 acc:0.832215 pre:0.843221 recall:0.832215\n",
      "[487] loss: 4.744 f1:0.839504 acc:0.838926 pre:0.853541 recall:0.838926\n",
      "[488] loss: 4.761 f1:0.834036 acc:0.838926 pre:0.857567 recall:0.838926\n",
      "[489] loss: 4.753 f1:0.849367 acc:0.852349 pre:0.861063 recall:0.852349\n",
      "[490] loss: 4.757 f1:0.856896 acc:0.859060 pre:0.868420 recall:0.859060\n",
      "[491] loss: 4.747 f1:0.843657 acc:0.845638 pre:0.850604 recall:0.845638\n",
      "[492] loss: 4.749 f1:0.843093 acc:0.845638 pre:0.853929 recall:0.845638\n",
      "[493] loss: 4.748 f1:0.849686 acc:0.852349 pre:0.863233 recall:0.852349\n",
      "[494] loss: 4.748 f1:0.849018 acc:0.852349 pre:0.863780 recall:0.852349\n",
      "[495] loss: 4.749 f1:0.850059 acc:0.852349 pre:0.867455 recall:0.852349\n",
      "[496] loss: 4.743 f1:0.855863 acc:0.859060 pre:0.869108 recall:0.859060\n",
      "[497] loss: 4.745 f1:0.817445 acc:0.818792 pre:0.832834 recall:0.818792\n",
      "[498] loss: 4.751 f1:0.836764 acc:0.838926 pre:0.857978 recall:0.838926\n",
      "[499] loss: 4.749 f1:0.834287 acc:0.838926 pre:0.851790 recall:0.838926\n",
      "[500] loss: 4.748 f1:0.849686 acc:0.852349 pre:0.863233 recall:0.852349\n",
      "[501] loss: 4.756 f1:0.829577 acc:0.832215 pre:0.842944 recall:0.832215\n",
      "[502] loss: 4.755 f1:0.843093 acc:0.845638 pre:0.853929 recall:0.845638\n",
      "[503] loss: 4.745 f1:0.850171 acc:0.852349 pre:0.863538 recall:0.852349\n",
      "[504] loss: 4.744 f1:0.845520 acc:0.845638 pre:0.860264 recall:0.845638\n",
      "[505] loss: 4.748 f1:0.850931 acc:0.852349 pre:0.857711 recall:0.852349\n",
      "[506] loss: 4.746 f1:0.830749 acc:0.832215 pre:0.843244 recall:0.832215\n",
      "[507] loss: 4.756 f1:0.837053 acc:0.838926 pre:0.847775 recall:0.838926\n",
      "[508] loss: 4.732 f1:0.836891 acc:0.838926 pre:0.850857 recall:0.838926\n",
      "[509] loss: 4.749 f1:0.855863 acc:0.859060 pre:0.869108 recall:0.859060\n",
      "[510] loss: 4.753 f1:0.849686 acc:0.852349 pre:0.863233 recall:0.852349\n",
      "[511] loss: 4.738 f1:0.848480 acc:0.852349 pre:0.872841 recall:0.852349\n",
      "[512] loss: 4.750 f1:0.850931 acc:0.852349 pre:0.857711 recall:0.852349\n",
      "[513] loss: 4.742 f1:0.856321 acc:0.859060 pre:0.876618 recall:0.859060\n",
      "[514] loss: 4.735 f1:0.848121 acc:0.852349 pre:0.876392 recall:0.852349\n",
      "[515] loss: 4.742 f1:0.842230 acc:0.845638 pre:0.862867 recall:0.845638\n",
      "[516] loss: 4.751 f1:0.849112 acc:0.852349 pre:0.863558 recall:0.852349\n",
      "[517] loss: 4.736 f1:0.844350 acc:0.845638 pre:0.849560 recall:0.845638\n",
      "[518] loss: 4.738 f1:0.844055 acc:0.845638 pre:0.854230 recall:0.845638\n",
      "[519] loss: 4.739 f1:0.843976 acc:0.845638 pre:0.850526 recall:0.845638\n",
      "[520] loss: 4.749 f1:0.836488 acc:0.838926 pre:0.844962 recall:0.838926\n",
      "[521] loss: 4.748 f1:0.856896 acc:0.859060 pre:0.868420 recall:0.859060\n",
      "[522] loss: 4.742 f1:0.831879 acc:0.832215 pre:0.842773 recall:0.832215\n",
      "[523] loss: 4.749 f1:0.863608 acc:0.865772 pre:0.874388 recall:0.865772\n",
      "[524] loss: 4.743 f1:0.842773 acc:0.845638 pre:0.857553 recall:0.845638\n",
      "[525] loss: 4.738 f1:0.856896 acc:0.859060 pre:0.868420 recall:0.859060\n",
      "[526] loss: 4.743 f1:0.856896 acc:0.859060 pre:0.868420 recall:0.859060\n",
      "[527] loss: 4.744 f1:0.837557 acc:0.838926 pre:0.841699 recall:0.838926\n",
      "[528] loss: 4.741 f1:0.856726 acc:0.859060 pre:0.872616 recall:0.859060\n",
      "[529] loss: 4.740 f1:0.849686 acc:0.852349 pre:0.863233 recall:0.852349\n",
      "[530] loss: 4.726 f1:0.834422 acc:0.838926 pre:0.853451 recall:0.838926\n",
      "[531] loss: 4.738 f1:0.857446 acc:0.859060 pre:0.869998 recall:0.859060\n",
      "[532] loss: 4.745 f1:0.862689 acc:0.865772 pre:0.875453 recall:0.865772\n",
      "[533] loss: 4.742 f1:0.838349 acc:0.838926 pre:0.861723 recall:0.838926\n",
      "[534] loss: 4.738 f1:0.838053 acc:0.838926 pre:0.849224 recall:0.838926\n",
      "[535] loss: 4.743 f1:0.849889 acc:0.852349 pre:0.863879 recall:0.852349\n",
      "[536] loss: 4.729 f1:0.830828 acc:0.832215 pre:0.835960 recall:0.832215\n",
      "[537] loss: 4.742 f1:0.862447 acc:0.865772 pre:0.883344 recall:0.865772\n",
      "[538] loss: 4.740 f1:0.832503 acc:0.838926 pre:0.858461 recall:0.838926\n",
      "[539] loss: 4.731 f1:0.850059 acc:0.852349 pre:0.867455 recall:0.852349\n",
      "[540] loss: 4.729 f1:0.864080 acc:0.865772 pre:0.875275 recall:0.865772\n",
      "[541] loss: 4.729 f1:0.837923 acc:0.838926 pre:0.842819 recall:0.838926\n",
      "[542] loss: 4.727 f1:0.838053 acc:0.838926 pre:0.848694 recall:0.838926\n",
      "[543] loss: 4.732 f1:0.836170 acc:0.838926 pre:0.851599 recall:0.838926\n",
      "[544] loss: 4.731 f1:0.850767 acc:0.852349 pre:0.863212 recall:0.852349\n",
      "[545] loss: 4.723 f1:0.864657 acc:0.865772 pre:0.879258 recall:0.865772\n",
      "[546] loss: 4.743 f1:0.796940 acc:0.798658 pre:0.801202 recall:0.798658\n",
      "[547] loss: 4.739 f1:0.841735 acc:0.845638 pre:0.858246 recall:0.845638\n",
      "[548] loss: 4.728 f1:0.858034 acc:0.859060 pre:0.861325 recall:0.859060\n",
      "[549] loss: 4.736 f1:0.838077 acc:0.838926 pre:0.840530 recall:0.838926\n",
      "[550] loss: 4.723 f1:0.869908 acc:0.872483 pre:0.880561 recall:0.872483\n",
      "[551] loss: 4.733 f1:0.856947 acc:0.859060 pre:0.865204 recall:0.859060\n",
      "[552] loss: 4.728 f1:0.870156 acc:0.872483 pre:0.883551 recall:0.872483\n",
      "[553] loss: 4.731 f1:0.850931 acc:0.852349 pre:0.857711 recall:0.852349\n",
      "[554] loss: 4.730 f1:0.837067 acc:0.838926 pre:0.852023 recall:0.838926\n",
      "[555] loss: 4.726 f1:0.850608 acc:0.852349 pre:0.865126 recall:0.852349\n",
      "[556] loss: 4.733 f1:0.844594 acc:0.845638 pre:0.851159 recall:0.845638\n",
      "[557] loss: 4.739 f1:0.845169 acc:0.845638 pre:0.854627 recall:0.845638\n",
      "[558] loss: 4.723 f1:0.870280 acc:0.872483 pre:0.881803 recall:0.872483\n",
      "[559] loss: 4.732 f1:0.856938 acc:0.859060 pre:0.868578 recall:0.859060\n",
      "[560] loss: 4.732 f1:0.855159 acc:0.859060 pre:0.873938 recall:0.859060\n",
      "[561] loss: 4.739 f1:0.856221 acc:0.859060 pre:0.872909 recall:0.859060\n",
      "[562] loss: 4.738 f1:0.838243 acc:0.838926 pre:0.839898 recall:0.838926\n",
      "[563] loss: 4.723 f1:0.836773 acc:0.838926 pre:0.857715 recall:0.838926\n",
      "[564] loss: 4.738 f1:0.833286 acc:0.832215 pre:0.854839 recall:0.832215\n",
      "[565] loss: 4.747 f1:0.849768 acc:0.852349 pre:0.863569 recall:0.852349\n",
      "[566] loss: 4.726 f1:0.856896 acc:0.859060 pre:0.868420 recall:0.859060\n",
      "[567] loss: 4.729 f1:0.843982 acc:0.845638 pre:0.862756 recall:0.845638\n",
      "[568] loss: 4.726 f1:0.843690 acc:0.845638 pre:0.855469 recall:0.845638\n",
      "[569] loss: 4.726 f1:0.857537 acc:0.859060 pre:0.862913 recall:0.859060\n",
      "[570] loss: 4.731 f1:0.857299 acc:0.859060 pre:0.868389 recall:0.859060\n",
      "[571] loss: 4.736 f1:0.870199 acc:0.872483 pre:0.883709 recall:0.872483\n",
      "[572] loss: 4.736 f1:0.845380 acc:0.845638 pre:0.853270 recall:0.845638\n",
      "[573] loss: 4.724 f1:0.849762 acc:0.852349 pre:0.859740 recall:0.852349\n",
      "[574] loss: 4.736 f1:0.831981 acc:0.832215 pre:0.846051 recall:0.832215\n",
      "[575] loss: 4.724 f1:0.821378 acc:0.818792 pre:0.840587 recall:0.818792\n",
      "[576] loss: 4.729 f1:0.850174 acc:0.852349 pre:0.862961 recall:0.852349\n",
      "[577] loss: 4.727 f1:0.857455 acc:0.859060 pre:0.868714 recall:0.859060\n",
      "[578] loss: 4.731 f1:0.863565 acc:0.865772 pre:0.874230 recall:0.865772\n",
      "[579] loss: 4.732 f1:0.863813 acc:0.865772 pre:0.869985 recall:0.865772\n",
      "[580] loss: 4.739 f1:0.851262 acc:0.852349 pre:0.860214 recall:0.852349\n",
      "[581] loss: 4.721 f1:0.857937 acc:0.859060 pre:0.868496 recall:0.859060\n",
      "[582] loss: 4.720 f1:0.851109 acc:0.852349 pre:0.857603 recall:0.852349\n",
      "[583] loss: 4.724 f1:0.863081 acc:0.865772 pre:0.874217 recall:0.865772\n",
      "[584] loss: 4.720 f1:0.864951 acc:0.865772 pre:0.880268 recall:0.865772\n",
      "[585] loss: 4.720 f1:0.850174 acc:0.852349 pre:0.862961 recall:0.852349\n",
      "[586] loss: 4.723 f1:0.857232 acc:0.859060 pre:0.865602 recall:0.859060\n",
      "[587] loss: 4.724 f1:0.864028 acc:0.865772 pre:0.878343 recall:0.865772\n",
      "[588] loss: 4.719 f1:0.863872 acc:0.865772 pre:0.877896 recall:0.865772\n",
      "[589] loss: 4.728 f1:0.863469 acc:0.865772 pre:0.877927 recall:0.865772\n",
      "[590] loss: 4.721 f1:0.831125 acc:0.832215 pre:0.861709 recall:0.832215\n",
      "[591] loss: 4.724 f1:0.864096 acc:0.865772 pre:0.870853 recall:0.865772\n",
      "[592] loss: 4.738 f1:0.863081 acc:0.865772 pre:0.874217 recall:0.865772\n",
      "[593] loss: 4.730 f1:0.838053 acc:0.838926 pre:0.848694 recall:0.838926\n",
      "[594] loss: 4.720 f1:0.864520 acc:0.865772 pre:0.875348 recall:0.865772\n",
      "[595] loss: 4.714 f1:0.864266 acc:0.865772 pre:0.871068 recall:0.865772\n",
      "[596] loss: 4.732 f1:0.841269 acc:0.845638 pre:0.858504 recall:0.845638\n",
      "[597] loss: 4.721 f1:0.864563 acc:0.865772 pre:0.874329 recall:0.865772\n",
      "[598] loss: 4.722 f1:0.857427 acc:0.859060 pre:0.865043 recall:0.859060\n",
      "[599] loss: 4.722 f1:0.830923 acc:0.832215 pre:0.837438 recall:0.832215\n",
      "[600] loss: 4.722 f1:0.863702 acc:0.865772 pre:0.870796 recall:0.865772\n",
      "[601] loss: 4.719 f1:0.821507 acc:0.825503 pre:0.849393 recall:0.825503\n",
      "[602] loss: 4.711 f1:0.851028 acc:0.852349 pre:0.861906 recall:0.852349\n",
      "[603] loss: 4.724 f1:0.845154 acc:0.845638 pre:0.852047 recall:0.845638\n",
      "[604] loss: 4.716 f1:0.857439 acc:0.859060 pre:0.862441 recall:0.859060\n",
      "[605] loss: 4.729 f1:0.857937 acc:0.859060 pre:0.868496 recall:0.859060\n",
      "[606] loss: 4.705 f1:0.877203 acc:0.879195 pre:0.886135 recall:0.879195\n",
      "[607] loss: 4.708 f1:0.864266 acc:0.865772 pre:0.871068 recall:0.865772\n",
      "[608] loss: 4.714 f1:0.869672 acc:0.872483 pre:0.883538 recall:0.872483\n",
      "[609] loss: 4.718 f1:0.848601 acc:0.852349 pre:0.864163 recall:0.852349\n",
      "[610] loss: 4.725 f1:0.863656 acc:0.865772 pre:0.874049 recall:0.865772\n",
      "[611] loss: 4.714 f1:0.869908 acc:0.872483 pre:0.880561 recall:0.872483\n",
      "[612] loss: 4.712 f1:0.842314 acc:0.845638 pre:0.857035 recall:0.845638\n",
      "[613] loss: 4.721 f1:0.855769 acc:0.859060 pre:0.873469 recall:0.859060\n",
      "[614] loss: 4.724 f1:0.856726 acc:0.859060 pre:0.872616 recall:0.859060\n",
      "[615] loss: 4.718 f1:0.877088 acc:0.879195 pre:0.885848 recall:0.879195\n",
      "[616] loss: 4.723 f1:0.850791 acc:0.852349 pre:0.856422 recall:0.852349\n",
      "[617] loss: 4.726 f1:0.862998 acc:0.865772 pre:0.876493 recall:0.865772\n",
      "[618] loss: 4.717 f1:0.870840 acc:0.872483 pre:0.877586 recall:0.872483\n",
      "[619] loss: 4.722 f1:0.864068 acc:0.865772 pre:0.873792 recall:0.865772\n",
      "[620] loss: 4.720 f1:0.851512 acc:0.852349 pre:0.860355 recall:0.852349\n",
      "[621] loss: 4.724 f1:0.844061 acc:0.845638 pre:0.853366 recall:0.845638\n",
      "[622] loss: 4.703 f1:0.857930 acc:0.859060 pre:0.873888 recall:0.859060\n",
      "[623] loss: 4.712 f1:0.870474 acc:0.872483 pre:0.884397 recall:0.872483\n",
      "[624] loss: 4.728 f1:0.862425 acc:0.865772 pre:0.878521 recall:0.865772\n",
      "[625] loss: 4.709 f1:0.871394 acc:0.872483 pre:0.889149 recall:0.872483\n",
      "[626] loss: 4.709 f1:0.841284 acc:0.845638 pre:0.861042 recall:0.845638\n",
      "[627] loss: 4.724 f1:0.864666 acc:0.865772 pre:0.873637 recall:0.865772\n",
      "[628] loss: 4.710 f1:0.851431 acc:0.852349 pre:0.859669 recall:0.852349\n",
      "[629] loss: 4.710 f1:0.845129 acc:0.845638 pre:0.855201 recall:0.845638\n",
      "[630] loss: 4.712 f1:0.845039 acc:0.845638 pre:0.854711 recall:0.845638\n",
      "[631] loss: 4.712 f1:0.851349 acc:0.852349 pre:0.863432 recall:0.852349\n",
      "[632] loss: 4.707 f1:0.857891 acc:0.859060 pre:0.872995 recall:0.859060\n",
      "[633] loss: 4.704 f1:0.844336 acc:0.845638 pre:0.854232 recall:0.845638\n",
      "[634] loss: 4.703 f1:0.870361 acc:0.872483 pre:0.884774 recall:0.872483\n",
      "[635] loss: 4.713 f1:0.890821 acc:0.892617 pre:0.904582 recall:0.892617\n",
      "[636] loss: 4.729 f1:0.870870 acc:0.872483 pre:0.876132 recall:0.872483\n",
      "[637] loss: 4.711 f1:0.845039 acc:0.845638 pre:0.854711 recall:0.845638\n",
      "[638] loss: 4.716 f1:0.877088 acc:0.879195 pre:0.885848 recall:0.879195\n",
      "[639] loss: 4.713 f1:0.831967 acc:0.832215 pre:0.841447 recall:0.832215\n",
      "[640] loss: 4.713 f1:0.850706 acc:0.852349 pre:0.854158 recall:0.852349\n",
      "[641] loss: 4.706 f1:0.870847 acc:0.872483 pre:0.879984 recall:0.872483\n",
      "[642] loss: 4.711 f1:0.857403 acc:0.859060 pre:0.860587 recall:0.859060\n",
      "[643] loss: 4.707 f1:0.825286 acc:0.825503 pre:0.835618 recall:0.825503\n",
      "[644] loss: 4.703 f1:0.877526 acc:0.879195 pre:0.887305 recall:0.879195\n",
      "[645] loss: 4.701 f1:0.834050 acc:0.838926 pre:0.858315 recall:0.838926\n",
      "[646] loss: 4.712 f1:0.884017 acc:0.885906 pre:0.893016 recall:0.885906\n",
      "[647] loss: 4.722 f1:0.844219 acc:0.845638 pre:0.848439 recall:0.845638\n",
      "[648] loss: 4.702 f1:0.871060 acc:0.872483 pre:0.882994 recall:0.872483\n",
      "[649] loss: 4.706 f1:0.857736 acc:0.859060 pre:0.865056 recall:0.859060\n",
      "[650] loss: 4.705 f1:0.857455 acc:0.859060 pre:0.868714 recall:0.859060\n",
      "[651] loss: 4.713 f1:0.857827 acc:0.859060 pre:0.865637 recall:0.859060\n",
      "[652] loss: 4.698 f1:0.844761 acc:0.845638 pre:0.862320 recall:0.845638\n",
      "[653] loss: 4.714 f1:0.857439 acc:0.859060 pre:0.862441 recall:0.859060\n",
      "[654] loss: 4.719 f1:0.890821 acc:0.892617 pre:0.904582 recall:0.892617\n",
      "[655] loss: 4.712 f1:0.863511 acc:0.865772 pre:0.873874 recall:0.865772\n",
      "[656] loss: 4.712 f1:0.811942 acc:0.812081 pre:0.824220 recall:0.812081\n",
      "[657] loss: 4.708 f1:0.856221 acc:0.859060 pre:0.868463 recall:0.859060\n",
      "[658] loss: 4.715 f1:0.877088 acc:0.879195 pre:0.885848 recall:0.879195\n",
      "[659] loss: 4.701 f1:0.844950 acc:0.845638 pre:0.867983 recall:0.845638\n",
      "[660] loss: 4.712 f1:0.842745 acc:0.845638 pre:0.853736 recall:0.845638\n",
      "[661] loss: 4.704 f1:0.831303 acc:0.832215 pre:0.838303 recall:0.832215\n",
      "[662] loss: 4.704 f1:0.871430 acc:0.872483 pre:0.882074 recall:0.872483\n",
      "[663] loss: 4.712 f1:0.871902 acc:0.872483 pre:0.888105 recall:0.872483\n",
      "[664] loss: 4.706 f1:0.871430 acc:0.872483 pre:0.882074 recall:0.872483\n",
      "[665] loss: 4.711 f1:0.840143 acc:0.845638 pre:0.870402 recall:0.845638\n",
      "[666] loss: 4.705 f1:0.870606 acc:0.872483 pre:0.877380 recall:0.872483\n",
      "[667] loss: 4.723 f1:0.844138 acc:0.845638 pre:0.851752 recall:0.845638\n",
      "[668] loss: 4.716 f1:0.851254 acc:0.852349 pre:0.855739 recall:0.852349\n",
      "[669] loss: 4.713 f1:0.863956 acc:0.865772 pre:0.868772 recall:0.865772\n",
      "[670] loss: 4.701 f1:0.854669 acc:0.859060 pre:0.875499 recall:0.859060\n",
      "[671] loss: 4.701 f1:0.863956 acc:0.865772 pre:0.868772 recall:0.865772\n",
      "[672] loss: 4.702 f1:0.804008 acc:0.812081 pre:0.845931 recall:0.812081\n",
      "[673] loss: 4.707 f1:0.850760 acc:0.852349 pre:0.854334 recall:0.852349\n",
      "[674] loss: 4.713 f1:0.844219 acc:0.845638 pre:0.848439 recall:0.845638\n",
      "[675] loss: 4.701 f1:0.837694 acc:0.838926 pre:0.843086 recall:0.838926\n",
      "[676] loss: 4.695 f1:0.871430 acc:0.872483 pre:0.882074 recall:0.872483\n",
      "[677] loss: 4.697 f1:0.850760 acc:0.852349 pre:0.854334 recall:0.852349\n",
      "[678] loss: 4.699 f1:0.837694 acc:0.838926 pre:0.843086 recall:0.838926\n",
      "[679] loss: 4.696 f1:0.837368 acc:0.838926 pre:0.843064 recall:0.838926\n",
      "[680] loss: 4.707 f1:0.851338 acc:0.852349 pre:0.857706 recall:0.852349\n",
      "[681] loss: 4.699 f1:0.870637 acc:0.872483 pre:0.877522 recall:0.872483\n",
      "[682] loss: 4.717 f1:0.844356 acc:0.845638 pre:0.853656 recall:0.845638\n",
      "[683] loss: 4.696 f1:0.844906 acc:0.845638 pre:0.851785 recall:0.845638\n",
      "[684] loss: 4.699 f1:0.870528 acc:0.872483 pre:0.877140 recall:0.872483\n",
      "[685] loss: 4.697 f1:0.857439 acc:0.859060 pre:0.862441 recall:0.859060\n",
      "[686] loss: 4.694 f1:0.844609 acc:0.845638 pre:0.850534 recall:0.845638\n",
      "[687] loss: 4.698 f1:0.863956 acc:0.865772 pre:0.868772 recall:0.865772\n",
      "[688] loss: 4.694 f1:0.843807 acc:0.845638 pre:0.852051 recall:0.845638\n",
      "[689] loss: 4.694 f1:0.857530 acc:0.859060 pre:0.863022 recall:0.859060\n",
      "[690] loss: 4.711 f1:0.826312 acc:0.832215 pre:0.860194 recall:0.832215\n",
      "[691] loss: 4.702 f1:0.837856 acc:0.838926 pre:0.846333 recall:0.838926\n",
      "[692] loss: 4.697 f1:0.844176 acc:0.845638 pre:0.847987 recall:0.845638\n",
      "[693] loss: 4.693 f1:0.849980 acc:0.852349 pre:0.859536 recall:0.852349\n",
      "[694] loss: 4.697 f1:0.877076 acc:0.879195 pre:0.885607 recall:0.879195\n",
      "[695] loss: 4.688 f1:0.857130 acc:0.859060 pre:0.862427 recall:0.859060\n",
      "[696] loss: 4.703 f1:0.869982 acc:0.872483 pre:0.882884 recall:0.872483\n",
      "[697] loss: 4.697 f1:0.837762 acc:0.838926 pre:0.843881 recall:0.838926\n",
      "[698] loss: 4.707 f1:0.877088 acc:0.879195 pre:0.885848 recall:0.879195\n",
      "[699] loss: 4.700 f1:0.864042 acc:0.865772 pre:0.868923 recall:0.865772\n",
      "[700] loss: 4.691 f1:0.864851 acc:0.865772 pre:0.877829 recall:0.865772\n",
      "[701] loss: 4.691 f1:0.857286 acc:0.859060 pre:0.868769 recall:0.859060\n",
      "[702] loss: 4.695 f1:0.844421 acc:0.845638 pre:0.848329 recall:0.845638\n",
      "[703] loss: 4.699 f1:0.838140 acc:0.838926 pre:0.844240 recall:0.838926\n",
      "[704] loss: 4.695 f1:0.831408 acc:0.832215 pre:0.842876 recall:0.832215\n",
      "[705] loss: 4.690 f1:0.863956 acc:0.865772 pre:0.868772 recall:0.865772\n",
      "[706] loss: 4.690 f1:0.863956 acc:0.865772 pre:0.868772 recall:0.865772\n",
      "[707] loss: 4.698 f1:0.845119 acc:0.845638 pre:0.858857 recall:0.845638\n",
      "[708] loss: 4.680 f1:0.851864 acc:0.852349 pre:0.860075 recall:0.852349\n",
      "[709] loss: 4.695 f1:0.863817 acc:0.865772 pre:0.870429 recall:0.865772\n",
      "[710] loss: 4.698 f1:0.837876 acc:0.838926 pre:0.846179 recall:0.838926\n",
      "[711] loss: 4.698 f1:0.851162 acc:0.852349 pre:0.854890 recall:0.852349\n",
      "[712] loss: 4.689 f1:0.831967 acc:0.832215 pre:0.841447 recall:0.832215\n",
      "[713] loss: 4.700 f1:0.870528 acc:0.872483 pre:0.877140 recall:0.872483\n",
      "[714] loss: 4.690 f1:0.837771 acc:0.838926 pre:0.845661 recall:0.838926\n",
      "[715] loss: 4.688 f1:0.844594 acc:0.845638 pre:0.851159 recall:0.845638\n",
      "[716] loss: 4.693 f1:0.863956 acc:0.865772 pre:0.868772 recall:0.865772\n",
      "[717] loss: 4.693 f1:0.830956 acc:0.832215 pre:0.840211 recall:0.832215\n",
      "[718] loss: 4.689 f1:0.869849 acc:0.872483 pre:0.884001 recall:0.872483\n",
      "[719] loss: 4.697 f1:0.831272 acc:0.832215 pre:0.837791 recall:0.832215\n",
      "[720] loss: 4.688 f1:0.870606 acc:0.872483 pre:0.877380 recall:0.872483\n",
      "[721] loss: 4.694 f1:0.870344 acc:0.872483 pre:0.879878 recall:0.872483\n",
      "[722] loss: 4.690 f1:0.863098 acc:0.865772 pre:0.877774 recall:0.865772\n",
      "[723] loss: 4.696 f1:0.838236 acc:0.838926 pre:0.846061 recall:0.838926\n",
      "[724] loss: 4.702 f1:0.845108 acc:0.845638 pre:0.851310 recall:0.845638\n",
      "[725] loss: 4.689 f1:0.863956 acc:0.865772 pre:0.868772 recall:0.865772\n",
      "[726] loss: 4.691 f1:0.863956 acc:0.865772 pre:0.868772 recall:0.865772\n",
      "[727] loss: 4.684 f1:0.877288 acc:0.879195 pre:0.885573 recall:0.879195\n",
      "[728] loss: 4.684 f1:0.865280 acc:0.865772 pre:0.871488 recall:0.865772\n",
      "[729] loss: 4.690 f1:0.877480 acc:0.879195 pre:0.887342 recall:0.879195\n",
      "[730] loss: 4.689 f1:0.838547 acc:0.838926 pre:0.846813 recall:0.838926\n",
      "[731] loss: 4.703 f1:0.863702 acc:0.865772 pre:0.870796 recall:0.865772\n",
      "[732] loss: 4.691 f1:0.838901 acc:0.838926 pre:0.848150 recall:0.838926\n",
      "[733] loss: 4.686 f1:0.808222 acc:0.812081 pre:0.820007 recall:0.812081\n",
      "[734] loss: 4.695 f1:0.817875 acc:0.818792 pre:0.828852 recall:0.818792\n",
      "[735] loss: 4.691 f1:0.858076 acc:0.859060 pre:0.862817 recall:0.859060\n",
      "[736] loss: 4.692 f1:0.870645 acc:0.872483 pre:0.881090 recall:0.872483\n",
      "[737] loss: 4.686 f1:0.865197 acc:0.865772 pre:0.868624 recall:0.865772\n",
      "[738] loss: 4.682 f1:0.850350 acc:0.852349 pre:0.856662 recall:0.852349\n",
      "[739] loss: 4.687 f1:0.869982 acc:0.872483 pre:0.882884 recall:0.872483\n",
      "[740] loss: 4.686 f1:0.857538 acc:0.859060 pre:0.862988 recall:0.859060\n",
      "[741] loss: 4.692 f1:0.837734 acc:0.838926 pre:0.841027 recall:0.838926\n",
      "[742] loss: 4.682 f1:0.830342 acc:0.832215 pre:0.840994 recall:0.832215\n",
      "[743] loss: 4.693 f1:0.849983 acc:0.852349 pre:0.866293 recall:0.852349\n",
      "[744] loss: 4.685 f1:0.831799 acc:0.832215 pre:0.840060 recall:0.832215\n",
      "[745] loss: 4.695 f1:0.890706 acc:0.892617 pre:0.904295 recall:0.892617\n",
      "[746] loss: 4.694 f1:0.805399 acc:0.805369 pre:0.819348 recall:0.805369\n",
      "[747] loss: 4.686 f1:0.825448 acc:0.825503 pre:0.835640 recall:0.825503\n",
      "[748] loss: 4.682 f1:0.863395 acc:0.865772 pre:0.873587 recall:0.865772\n",
      "[749] loss: 4.690 f1:0.857149 acc:0.859060 pre:0.869374 recall:0.859060\n",
      "[750] loss: 4.683 f1:0.850439 acc:0.852349 pre:0.858601 recall:0.852349\n",
      "[751] loss: 4.680 f1:0.850263 acc:0.852349 pre:0.856511 recall:0.852349\n",
      "[752] loss: 4.692 f1:0.856920 acc:0.859060 pre:0.864730 recall:0.859060\n",
      "[753] loss: 4.690 f1:0.834607 acc:0.838926 pre:0.854149 recall:0.838926\n",
      "[754] loss: 4.682 f1:0.864787 acc:0.865772 pre:0.882981 recall:0.865772\n",
      "[755] loss: 4.678 f1:0.864443 acc:0.865772 pre:0.869764 recall:0.865772\n",
      "[756] loss: 4.695 f1:0.844523 acc:0.845638 pre:0.855184 recall:0.845638\n",
      "[757] loss: 4.688 f1:0.857588 acc:0.859060 pre:0.863008 recall:0.859060\n",
      "[758] loss: 4.691 f1:0.829804 acc:0.832215 pre:0.852323 recall:0.832215\n",
      "[759] loss: 4.691 f1:0.825865 acc:0.825503 pre:0.836783 recall:0.825503\n",
      "[760] loss: 4.685 f1:0.856900 acc:0.859060 pre:0.862599 recall:0.859060\n",
      "[761] loss: 4.682 f1:0.883777 acc:0.885906 pre:0.895051 recall:0.885906\n",
      "[762] loss: 4.685 f1:0.857530 acc:0.859060 pre:0.863022 recall:0.859060\n",
      "[763] loss: 4.686 f1:0.824634 acc:0.825503 pre:0.834744 recall:0.825503\n",
      "[764] loss: 4.689 f1:0.857130 acc:0.859060 pre:0.862427 recall:0.859060\n",
      "[765] loss: 4.689 f1:0.857538 acc:0.859060 pre:0.862988 recall:0.859060\n",
      "[766] loss: 4.691 f1:0.870528 acc:0.872483 pre:0.877140 recall:0.872483\n",
      "[767] loss: 4.688 f1:0.878144 acc:0.879195 pre:0.881014 recall:0.879195\n",
      "[768] loss: 4.681 f1:0.850350 acc:0.852349 pre:0.856662 recall:0.852349\n",
      "[769] loss: 4.686 f1:0.850263 acc:0.852349 pre:0.856511 recall:0.852349\n",
      "[770] loss: 4.690 f1:0.877185 acc:0.879195 pre:0.885989 recall:0.879195\n",
      "[771] loss: 4.691 f1:0.850120 acc:0.852349 pre:0.860552 recall:0.852349\n",
      "[772] loss: 4.675 f1:0.837247 acc:0.838926 pre:0.845983 recall:0.838926\n",
      "[773] loss: 4.683 f1:0.877088 acc:0.879195 pre:0.885848 recall:0.879195\n",
      "[774] loss: 4.698 f1:0.858108 acc:0.859060 pre:0.861093 recall:0.859060\n",
      "[775] loss: 4.686 f1:0.844977 acc:0.845638 pre:0.849058 recall:0.845638\n",
      "[776] loss: 4.691 f1:0.831336 acc:0.832215 pre:0.837989 recall:0.832215\n",
      "[777] loss: 4.682 f1:0.864461 acc:0.865772 pre:0.871823 recall:0.865772\n",
      "[778] loss: 4.684 f1:0.870261 acc:0.872483 pre:0.879503 recall:0.872483\n",
      "[779] loss: 4.684 f1:0.864245 acc:0.865772 pre:0.871421 recall:0.865772\n",
      "[780] loss: 4.685 f1:0.827084 acc:0.825503 pre:0.840523 recall:0.825503\n",
      "[781] loss: 4.675 f1:0.852068 acc:0.852349 pre:0.866534 recall:0.852349\n",
      "[782] loss: 4.677 f1:0.851078 acc:0.852349 pre:0.857234 recall:0.852349\n",
      "[783] loss: 4.692 f1:0.844774 acc:0.845638 pre:0.849536 recall:0.845638\n",
      "[784] loss: 4.672 f1:0.838270 acc:0.838926 pre:0.846996 recall:0.838926\n",
      "[785] loss: 4.685 f1:0.864605 acc:0.865772 pre:0.870823 recall:0.865772\n",
      "[786] loss: 4.696 f1:0.824537 acc:0.825503 pre:0.833422 recall:0.825503\n",
      "[787] loss: 4.712 f1:0.863956 acc:0.865772 pre:0.868772 recall:0.865772\n",
      "[788] loss: 4.704 f1:0.870360 acc:0.872483 pre:0.879999 recall:0.872483\n",
      "[789] loss: 4.688 f1:0.877088 acc:0.879195 pre:0.885848 recall:0.879195\n",
      "[790] loss: 4.687 f1:0.863684 acc:0.865772 pre:0.871379 recall:0.865772\n",
      "[791] loss: 4.683 f1:0.844424 acc:0.845638 pre:0.852571 recall:0.845638\n",
      "[792] loss: 4.674 f1:0.863028 acc:0.865772 pre:0.877449 recall:0.865772\n",
      "[793] loss: 4.676 f1:0.885165 acc:0.885906 pre:0.887749 recall:0.885906\n",
      "[794] loss: 4.688 f1:0.838048 acc:0.838926 pre:0.844700 recall:0.838926\n",
      "[795] loss: 4.678 f1:0.870592 acc:0.872483 pre:0.880805 recall:0.872483\n",
      "[796] loss: 4.705 f1:0.815851 acc:0.812081 pre:0.859958 recall:0.812081\n",
      "[797] loss: 4.676 f1:0.855564 acc:0.859060 pre:0.872767 recall:0.859060\n",
      "[798] loss: 4.690 f1:0.877157 acc:0.879195 pre:0.886173 recall:0.879195\n",
      "[799] loss: 4.677 f1:0.845212 acc:0.845638 pre:0.851916 recall:0.845638\n",
      "[800] loss: 4.675 f1:0.877582 acc:0.879195 pre:0.886518 recall:0.879195\n",
      "[801] loss: 4.703 f1:0.857439 acc:0.859060 pre:0.862441 recall:0.859060\n",
      "[802] loss: 4.692 f1:0.856746 acc:0.859060 pre:0.862391 recall:0.859060\n",
      "[803] loss: 4.682 f1:0.851482 acc:0.852349 pre:0.856894 recall:0.852349\n",
      "[804] loss: 4.689 f1:0.857182 acc:0.859060 pre:0.866017 recall:0.859060\n",
      "[805] loss: 4.691 f1:0.831720 acc:0.832215 pre:0.839980 recall:0.832215\n",
      "[806] loss: 4.680 f1:0.864042 acc:0.865772 pre:0.868923 recall:0.865772\n",
      "[807] loss: 4.681 f1:0.870535 acc:0.872483 pre:0.877338 recall:0.872483\n",
      "[808] loss: 4.693 f1:0.864278 acc:0.865772 pre:0.869941 recall:0.865772\n",
      "[809] loss: 4.682 f1:0.838959 acc:0.838926 pre:0.847320 recall:0.838926\n",
      "[810] loss: 4.675 f1:0.844219 acc:0.845638 pre:0.848439 recall:0.845638\n",
      "[811] loss: 4.679 f1:0.850628 acc:0.852349 pre:0.857480 recall:0.852349\n",
      "[812] loss: 4.678 f1:0.863956 acc:0.865772 pre:0.868772 recall:0.865772\n",
      "[813] loss: 4.700 f1:0.850263 acc:0.852349 pre:0.856511 recall:0.852349\n",
      "[814] loss: 4.673 f1:0.877418 acc:0.879195 pre:0.887149 recall:0.879195\n",
      "[815] loss: 4.681 f1:0.879113 acc:0.879195 pre:0.882731 recall:0.879195\n",
      "[816] loss: 4.678 f1:0.844956 acc:0.845638 pre:0.850886 recall:0.845638\n",
      "[817] loss: 4.676 f1:0.857313 acc:0.859060 pre:0.863199 recall:0.859060\n",
      "[818] loss: 4.696 f1:0.862812 acc:0.865772 pre:0.877784 recall:0.865772\n",
      "[819] loss: 4.678 f1:0.863702 acc:0.865772 pre:0.870796 recall:0.865772\n",
      "[820] loss: 4.676 f1:0.865737 acc:0.865772 pre:0.870349 recall:0.865772\n",
      "[821] loss: 4.689 f1:0.878094 acc:0.879195 pre:0.880682 recall:0.879195\n",
      "[822] loss: 4.695 f1:0.864056 acc:0.865772 pre:0.869251 recall:0.865772\n",
      "[823] loss: 4.695 f1:0.858522 acc:0.859060 pre:0.863623 recall:0.859060\n",
      "[824] loss: 4.687 f1:0.870770 acc:0.872483 pre:0.875653 recall:0.872483\n",
      "[825] loss: 4.670 f1:0.863395 acc:0.865772 pre:0.873587 recall:0.865772\n",
      "[826] loss: 4.680 f1:0.842064 acc:0.845638 pre:0.857609 recall:0.845638\n",
      "[827] loss: 4.682 f1:0.851012 acc:0.852349 pre:0.857635 recall:0.852349\n",
      "[828] loss: 4.677 f1:0.870235 acc:0.872483 pre:0.879496 recall:0.872483\n",
      "[829] loss: 4.670 f1:0.863927 acc:0.865772 pre:0.868602 recall:0.865772\n",
      "[830] loss: 4.676 f1:0.863512 acc:0.865772 pre:0.872029 recall:0.865772\n",
      "[831] loss: 4.677 f1:0.837994 acc:0.838926 pre:0.848927 recall:0.838926\n",
      "[832] loss: 4.681 f1:0.863726 acc:0.865772 pre:0.874888 recall:0.865772\n",
      "[833] loss: 4.680 f1:0.865395 acc:0.865772 pre:0.870271 recall:0.865772\n",
      "[834] loss: 4.680 f1:0.864059 acc:0.865772 pre:0.872532 recall:0.865772\n",
      "[835] loss: 4.679 f1:0.848258 acc:0.852349 pre:0.874442 recall:0.852349\n",
      "[836] loss: 4.678 f1:0.865373 acc:0.865772 pre:0.868845 recall:0.865772\n",
      "[837] loss: 4.672 f1:0.849318 acc:0.852349 pre:0.864439 recall:0.852349\n",
      "[838] loss: 4.685 f1:0.885400 acc:0.885906 pre:0.886141 recall:0.885906\n",
      "[839] loss: 4.673 f1:0.857858 acc:0.859060 pre:0.862779 recall:0.859060\n",
      "[840] loss: 4.684 f1:0.851606 acc:0.852349 pre:0.860755 recall:0.852349\n",
      "[841] loss: 4.681 f1:0.845314 acc:0.845638 pre:0.854625 recall:0.845638\n",
      "[842] loss: 4.680 f1:0.863757 acc:0.865772 pre:0.875846 recall:0.865772\n",
      "[843] loss: 4.677 f1:0.864452 acc:0.865772 pre:0.868753 recall:0.865772\n",
      "[844] loss: 4.671 f1:0.852241 acc:0.852349 pre:0.857306 recall:0.852349\n",
      "[845] loss: 4.684 f1:0.878992 acc:0.879195 pre:0.883667 recall:0.879195\n",
      "[846] loss: 4.669 f1:0.864452 acc:0.865772 pre:0.868753 recall:0.865772\n",
      "[847] loss: 4.671 f1:0.884881 acc:0.885906 pre:0.889142 recall:0.885906\n",
      "[848] loss: 4.668 f1:0.891369 acc:0.892617 pre:0.897769 recall:0.892617\n",
      "[849] loss: 4.675 f1:0.871977 acc:0.872483 pre:0.873830 recall:0.872483\n",
      "[850] loss: 4.671 f1:0.878626 acc:0.879195 pre:0.879580 recall:0.879195\n",
      "[851] loss: 4.661 f1:0.877663 acc:0.879195 pre:0.884755 recall:0.879195\n",
      "[852] loss: 4.678 f1:0.872571 acc:0.872483 pre:0.877609 recall:0.872483\n",
      "[853] loss: 4.692 f1:0.885087 acc:0.885906 pre:0.886877 recall:0.885906\n",
      "[854] loss: 4.680 f1:0.842806 acc:0.845638 pre:0.852539 recall:0.845638\n",
      "[855] loss: 4.675 f1:0.884232 acc:0.885906 pre:0.894968 recall:0.885906\n",
      "[856] loss: 4.667 f1:0.878906 acc:0.879195 pre:0.882644 recall:0.879195\n",
      "[857] loss: 4.663 f1:0.871302 acc:0.872483 pre:0.876606 recall:0.872483\n",
      "[858] loss: 4.666 f1:0.871513 acc:0.872483 pre:0.876077 recall:0.872483\n",
      "[859] loss: 4.686 f1:0.876122 acc:0.879195 pre:0.894420 recall:0.879195\n",
      "[860] loss: 4.665 f1:0.865952 acc:0.865772 pre:0.872833 recall:0.865772\n",
      "[861] loss: 4.674 f1:0.878709 acc:0.879195 pre:0.883181 recall:0.879195\n",
      "[862] loss: 4.669 f1:0.885164 acc:0.885906 pre:0.886994 recall:0.885906\n",
      "[863] loss: 4.667 f1:0.878144 acc:0.879195 pre:0.881014 recall:0.879195\n",
      "[864] loss: 4.669 f1:0.892143 acc:0.892617 pre:0.894182 recall:0.892617\n",
      "[865] loss: 4.666 f1:0.884955 acc:0.885906 pre:0.888454 recall:0.885906\n",
      "[866] loss: 4.676 f1:0.878144 acc:0.879195 pre:0.881014 recall:0.879195\n",
      "[867] loss: 4.665 f1:0.878764 acc:0.879195 pre:0.880115 recall:0.879195\n",
      "[868] loss: 4.674 f1:0.885114 acc:0.885906 pre:0.887417 recall:0.885906\n",
      "[869] loss: 4.671 f1:0.859777 acc:0.859060 pre:0.868501 recall:0.859060\n",
      "[870] loss: 4.690 f1:0.872143 acc:0.872483 pre:0.875771 recall:0.872483\n",
      "[871] loss: 4.670 f1:0.879113 acc:0.879195 pre:0.882731 recall:0.879195\n",
      "[872] loss: 4.669 f1:0.885209 acc:0.885906 pre:0.887477 recall:0.885906\n",
      "[873] loss: 4.682 f1:0.885165 acc:0.885906 pre:0.887749 recall:0.885906\n",
      "[874] loss: 4.683 f1:0.870419 acc:0.872483 pre:0.884445 recall:0.872483\n",
      "[875] loss: 4.666 f1:0.871318 acc:0.872483 pre:0.874669 recall:0.872483\n",
      "[876] loss: 4.666 f1:0.879000 acc:0.879195 pre:0.883634 recall:0.879195\n",
      "[877] loss: 4.674 f1:0.892021 acc:0.892617 pre:0.893582 recall:0.892617\n",
      "[878] loss: 4.663 f1:0.898398 acc:0.899329 pre:0.903438 recall:0.899329\n",
      "[879] loss: 4.666 f1:0.878778 acc:0.879195 pre:0.882200 recall:0.879195\n",
      "[880] loss: 4.677 f1:0.898655 acc:0.899329 pre:0.901934 recall:0.899329\n",
      "[881] loss: 4.655 f1:0.871424 acc:0.872483 pre:0.877207 recall:0.872483\n",
      "[882] loss: 4.655 f1:0.885514 acc:0.885906 pre:0.889508 recall:0.885906\n",
      "[883] loss: 4.661 f1:0.872009 acc:0.872483 pre:0.878087 recall:0.872483\n",
      "[884] loss: 4.671 f1:0.866576 acc:0.865772 pre:0.878282 recall:0.865772\n",
      "[885] loss: 4.685 f1:0.844500 acc:0.845638 pre:0.849535 recall:0.845638\n",
      "[886] loss: 4.661 f1:0.891355 acc:0.892617 pre:0.897016 recall:0.892617\n",
      "[887] loss: 4.667 f1:0.885457 acc:0.885906 pre:0.887442 recall:0.885906\n",
      "[888] loss: 4.659 f1:0.885799 acc:0.885906 pre:0.891164 recall:0.885906\n",
      "[889] loss: 4.664 f1:0.877088 acc:0.879195 pre:0.885848 recall:0.879195\n",
      "[890] loss: 4.675 f1:0.865315 acc:0.865772 pre:0.872601 recall:0.865772\n",
      "[891] loss: 4.664 f1:0.864767 acc:0.865772 pre:0.871084 recall:0.865772\n",
      "[892] loss: 4.671 f1:0.892183 acc:0.892617 pre:0.897959 recall:0.892617\n",
      "[893] loss: 4.659 f1:0.845420 acc:0.845638 pre:0.853134 recall:0.845638\n",
      "[894] loss: 4.671 f1:0.845272 acc:0.845638 pre:0.851269 recall:0.845638\n",
      "[895] loss: 4.671 f1:0.884228 acc:0.885906 pre:0.891197 recall:0.885906\n",
      "[896] loss: 4.668 f1:0.871302 acc:0.872483 pre:0.876606 recall:0.872483\n",
      "[897] loss: 4.671 f1:0.892143 acc:0.892617 pre:0.894182 recall:0.892617\n",
      "[898] loss: 4.667 f1:0.891621 acc:0.892617 pre:0.899690 recall:0.892617\n",
      "[899] loss: 4.659 f1:0.878459 acc:0.879195 pre:0.883345 recall:0.879195\n",
      "[900] loss: 4.660 f1:0.853155 acc:0.852349 pre:0.865815 recall:0.852349\n",
      "[901] loss: 4.669 f1:0.885749 acc:0.885906 pre:0.888757 recall:0.885906\n",
      "[902] loss: 4.675 f1:0.885749 acc:0.885906 pre:0.888757 recall:0.885906\n",
      "[903] loss: 4.661 f1:0.864888 acc:0.865772 pre:0.868669 recall:0.865772\n",
      "[904] loss: 4.667 f1:0.878459 acc:0.879195 pre:0.883345 recall:0.879195\n",
      "[905] loss: 4.665 f1:0.891934 acc:0.892617 pre:0.895536 recall:0.892617\n",
      "[906] loss: 4.691 f1:0.865999 acc:0.865772 pre:0.871520 recall:0.865772\n",
      "[907] loss: 4.664 f1:0.870188 acc:0.872483 pre:0.882729 recall:0.872483\n",
      "[908] loss: 4.664 f1:0.885239 acc:0.885906 pre:0.886965 recall:0.885906\n",
      "[909] loss: 4.664 f1:0.893079 acc:0.892617 pre:0.905667 recall:0.892617\n",
      "[910] loss: 4.661 f1:0.858873 acc:0.859060 pre:0.864735 recall:0.859060\n",
      "[911] loss: 4.657 f1:0.885440 acc:0.885906 pre:0.890914 recall:0.885906\n",
      "[912] loss: 4.664 f1:0.852507 acc:0.852349 pre:0.858019 recall:0.852349\n",
      "[913] loss: 4.663 f1:0.865393 acc:0.865772 pre:0.869449 recall:0.865772\n",
      "[914] loss: 4.659 f1:0.865756 acc:0.865772 pre:0.870922 recall:0.865772\n",
      "[915] loss: 4.655 f1:0.859450 acc:0.859060 pre:0.865427 recall:0.859060\n",
      "[916] loss: 4.665 f1:0.856495 acc:0.859060 pre:0.874502 recall:0.859060\n",
      "[917] loss: 4.674 f1:0.885535 acc:0.885906 pre:0.887041 recall:0.885906\n",
      "[918] loss: 4.661 f1:0.871379 acc:0.872483 pre:0.876723 recall:0.872483\n",
      "[919] loss: 4.659 f1:0.872062 acc:0.872483 pre:0.874223 recall:0.872483\n",
      "[920] loss: 4.665 f1:0.885015 acc:0.885906 pre:0.888709 recall:0.885906\n",
      "[921] loss: 4.650 f1:0.837774 acc:0.838926 pre:0.846124 recall:0.838926\n",
      "[922] loss: 4.665 f1:0.871977 acc:0.872483 pre:0.873830 recall:0.872483\n",
      "[923] loss: 4.656 f1:0.864335 acc:0.865772 pre:0.873483 recall:0.865772\n",
      "[924] loss: 4.658 f1:0.885209 acc:0.885906 pre:0.887477 recall:0.885906\n",
      "[925] loss: 4.663 f1:0.872062 acc:0.872483 pre:0.874223 recall:0.872483\n",
      "[926] loss: 4.656 f1:0.877757 acc:0.879195 pre:0.882340 recall:0.879195\n",
      "[927] loss: 4.660 f1:0.898655 acc:0.899329 pre:0.901934 recall:0.899329\n",
      "[928] loss: 4.666 f1:0.891841 acc:0.892617 pre:0.895054 recall:0.892617\n",
      "[929] loss: 4.659 f1:0.878620 acc:0.879195 pre:0.883159 recall:0.879195\n",
      "[930] loss: 4.658 f1:0.885309 acc:0.885906 pre:0.887738 recall:0.885906\n",
      "[931] loss: 4.663 f1:0.878619 acc:0.879195 pre:0.883418 recall:0.879195\n",
      "[932] loss: 4.679 f1:0.862788 acc:0.865772 pre:0.877807 recall:0.865772\n",
      "[933] loss: 4.659 f1:0.858019 acc:0.859060 pre:0.868811 recall:0.859060\n",
      "[934] loss: 4.663 f1:0.846805 acc:0.845638 pre:0.866796 recall:0.845638\n",
      "[935] loss: 4.670 f1:0.865476 acc:0.865772 pre:0.869281 recall:0.865772\n",
      "[936] loss: 4.659 f1:0.844903 acc:0.845638 pre:0.853696 recall:0.845638\n",
      "[937] loss: 4.654 f1:0.884570 acc:0.885906 pre:0.891104 recall:0.885906\n",
      "[938] loss: 4.652 f1:0.891878 acc:0.892617 pre:0.894367 recall:0.892617\n",
      "[939] loss: 4.677 f1:0.897980 acc:0.899329 pre:0.909348 recall:0.899329\n",
      "[940] loss: 4.657 f1:0.872179 acc:0.872483 pre:0.876026 recall:0.872483\n",
      "[941] loss: 4.654 f1:0.878373 acc:0.879195 pre:0.881048 recall:0.879195\n",
      "[942] loss: 4.650 f1:0.891732 acc:0.892617 pre:0.895221 recall:0.892617\n",
      "[943] loss: 4.659 f1:0.885087 acc:0.885906 pre:0.886877 recall:0.885906\n",
      "[944] loss: 4.666 f1:0.885015 acc:0.885906 pre:0.888709 recall:0.885906\n",
      "[945] loss: 4.663 f1:0.884235 acc:0.885906 pre:0.891327 recall:0.885906\n",
      "[946] loss: 4.672 f1:0.891331 acc:0.892617 pre:0.896974 recall:0.892617\n",
      "[947] loss: 4.665 f1:0.885628 acc:0.885906 pre:0.890142 recall:0.885906\n",
      "[948] loss: 4.660 f1:0.892556 acc:0.892617 pre:0.896419 recall:0.892617\n",
      "[949] loss: 4.666 f1:0.892365 acc:0.892617 pre:0.894368 recall:0.892617\n",
      "[950] loss: 4.664 f1:0.870360 acc:0.872483 pre:0.879999 recall:0.872483\n",
      "[951] loss: 4.666 f1:0.891991 acc:0.892617 pre:0.896768 recall:0.892617\n",
      "[952] loss: 4.659 f1:0.872124 acc:0.872483 pre:0.878118 recall:0.872483\n",
      "[953] loss: 4.660 f1:0.891900 acc:0.892617 pre:0.894730 recall:0.892617\n",
      "[954] loss: 4.657 f1:0.878815 acc:0.879195 pre:0.882052 recall:0.879195\n",
      "[955] loss: 4.646 f1:0.892084 acc:0.892617 pre:0.895103 recall:0.892617\n",
      "[956] loss: 4.664 f1:0.878107 acc:0.879195 pre:0.885419 recall:0.879195\n",
      "[957] loss: 4.670 f1:0.898884 acc:0.899329 pre:0.900039 recall:0.899329\n",
      "[958] loss: 4.658 f1:0.849762 acc:0.852349 pre:0.863515 recall:0.852349\n",
      "[959] loss: 4.665 f1:0.891880 acc:0.892617 pre:0.894398 recall:0.892617\n",
      "[960] loss: 4.663 f1:0.891669 acc:0.892617 pre:0.896357 recall:0.892617\n",
      "[961] loss: 4.648 f1:0.872118 acc:0.872483 pre:0.876905 recall:0.872483\n",
      "[962] loss: 4.662 f1:0.845470 acc:0.845638 pre:0.854125 recall:0.845638\n",
      "[963] loss: 4.653 f1:0.885170 acc:0.885906 pre:0.887502 recall:0.885906\n",
      "[964] loss: 4.654 f1:0.877654 acc:0.879195 pre:0.886808 recall:0.879195\n",
      "[965] loss: 4.653 f1:0.878754 acc:0.879195 pre:0.881331 recall:0.879195\n",
      "[966] loss: 4.668 f1:0.855922 acc:0.859060 pre:0.871034 recall:0.859060\n",
      "[967] loss: 4.647 f1:0.878754 acc:0.879195 pre:0.881331 recall:0.879195\n",
      "[968] loss: 4.653 f1:0.885167 acc:0.885906 pre:0.888278 recall:0.885906\n",
      "[969] loss: 4.659 f1:0.892070 acc:0.892617 pre:0.893158 recall:0.892617\n",
      "[970] loss: 4.652 f1:0.885268 acc:0.885906 pre:0.887154 recall:0.885906\n",
      "[971] loss: 4.654 f1:0.873249 acc:0.872483 pre:0.880574 recall:0.872483\n",
      "[972] loss: 4.669 f1:0.872612 acc:0.872483 pre:0.876754 recall:0.872483\n",
      "[973] loss: 4.661 f1:0.878289 acc:0.879195 pre:0.882616 recall:0.879195\n",
      "[974] loss: 4.654 f1:0.877599 acc:0.879195 pre:0.884635 recall:0.879195\n",
      "[975] loss: 4.659 f1:0.891776 acc:0.892617 pre:0.895001 recall:0.892617\n",
      "[976] loss: 4.645 f1:0.871955 acc:0.872483 pre:0.874773 recall:0.872483\n",
      "[977] loss: 4.674 f1:0.850900 acc:0.852349 pre:0.856281 recall:0.852349\n",
      "[978] loss: 4.660 f1:0.878467 acc:0.879195 pre:0.882183 recall:0.879195\n",
      "[979] loss: 4.639 f1:0.891781 acc:0.892617 pre:0.894798 recall:0.892617\n",
      "[980] loss: 4.659 f1:0.840584 acc:0.845638 pre:0.865111 recall:0.845638\n",
      "[981] loss: 4.660 f1:0.878416 acc:0.879195 pre:0.881877 recall:0.879195\n",
      "[982] loss: 4.666 f1:0.885913 acc:0.885906 pre:0.888687 recall:0.885906\n",
      "[983] loss: 4.656 f1:0.885502 acc:0.885906 pre:0.886311 recall:0.885906\n",
      "[984] loss: 4.663 f1:0.852629 acc:0.852349 pre:0.862945 recall:0.852349\n",
      "[985] loss: 4.655 f1:0.847138 acc:0.845638 pre:0.858726 recall:0.845638\n",
      "[986] loss: 4.651 f1:0.857761 acc:0.859060 pre:0.863020 recall:0.859060\n",
      "[987] loss: 4.661 f1:0.885749 acc:0.885906 pre:0.888757 recall:0.885906\n",
      "[988] loss: 4.646 f1:0.799760 acc:0.798658 pre:0.836045 recall:0.798658\n",
      "[989] loss: 4.657 f1:0.891331 acc:0.892617 pre:0.896974 recall:0.892617\n",
      "[990] loss: 4.661 f1:0.878017 acc:0.879195 pre:0.881977 recall:0.879195\n",
      "[991] loss: 4.650 f1:0.878834 acc:0.879195 pre:0.880239 recall:0.879195\n",
      "[992] loss: 4.644 f1:0.884490 acc:0.885906 pre:0.890863 recall:0.885906\n",
      "[993] loss: 4.637 f1:0.878898 acc:0.879195 pre:0.882677 recall:0.879195\n",
      "[994] loss: 4.640 f1:0.885457 acc:0.885906 pre:0.887442 recall:0.885906\n",
      "[995] loss: 4.642 f1:0.871843 acc:0.872483 pre:0.877399 recall:0.872483\n",
      "[996] loss: 4.650 f1:0.865476 acc:0.865772 pre:0.869281 recall:0.865772\n",
      "[997] loss: 4.648 f1:0.865791 acc:0.865772 pre:0.873212 recall:0.865772\n",
      "[998] loss: 4.637 f1:0.892064 acc:0.892617 pre:0.893906 recall:0.892617\n",
      "[999] loss: 4.649 f1:0.878640 acc:0.879195 pre:0.880606 recall:0.879195\n",
      "[1000] loss: 4.654 f1:0.891898 acc:0.892617 pre:0.894699 recall:0.892617\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score,f1_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from datetime import datetime\n",
    "import os\n",
    "loss_history = []\n",
    "loss_epoch_history = []\n",
    "# print(\"if gpu=\",torch.cuda.is_available())\n",
    "          \n",
    "# 训练循环示例\n",
    "lr_ = g_lr_inited\n",
    "\n",
    "last_modle_file_name = \"\"\n",
    "best_recall_modle_file_name = \"\"\n",
    "best_acc_modle_file_name = \"\"\n",
    "best_f1_modle_file_name = \"\"\n",
    "best_pre_modle_file_name = \"\"\n",
    "best_recall_score = 0.0\n",
    "best_acc_score = 0.0\n",
    "best_pre_score = 0.0\n",
    "best_f1_score = 0.0\n",
    "\n",
    "\n",
    "def epoch_test():\n",
    "    true_labels = []\n",
    "    predicted_labels = []\n",
    "    total_test_count = 0\n",
    "    total_no_person_count = 0         # 总共无人样本数量\n",
    "    no_person_as_person_count = 0     # 无人识别为有人数量\n",
    "    person_as_no_person_count = 0     # 有人识别为无数量\n",
    "    total_no_change_count = 0         # 低位时非站姿总数，高位代表非坐姿总数\n",
    "    change_err_count = 0              # 低位非站姿识别为站姿，高位非坐姿识别为站姿\n",
    "\n",
    "# Accuracy: 0.8792\n",
    "# Precision: 0.8829\n",
    "# Recall: 0.8792\n",
    "\n",
    "# labels are  ['idle', 'sit', 'sit2stand', 'stand', 'stand2sit']\n",
    "# Precision per class: [0.94852941 0.9        0.77966102 0.87121212 0.77083333]\n",
    "# Recall per class: [0.87755102 0.88636364 0.86792453 0.89147287 0.84090909]\n",
    "\n",
    "# 混淆矩阵:\n",
    "# [[129   6   0  11   1]\n",
    "#  [  1 117   7   1   6]\n",
    "#  [  0   3  46   1   3]\n",
    "#  [  5   4   4 115   1]\n",
    "#  [  1   0   2   4  37]]\n",
    "\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i, (IR_data, distance_data, labels, filename) in enumerate(test_dataloader):\n",
    "            IR_data = IR_data.to(mydevice)\n",
    "            distance_data = distance_data.to(mydevice)\n",
    "\n",
    "            outputs = net(IR_data, distance_data)\n",
    "            outputs = outputs.cpu()\n",
    "\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            _, gt = torch.max(labels.data, 1)\n",
    "            \n",
    "            true_labels.extend(gt.numpy())\n",
    "            predicted_labels.extend(predicted.numpy()) \n",
    "\n",
    "    res_acc = accuracy_score(true_labels, predicted_labels)\n",
    "    res_pre =  precision_score(true_labels, predicted_labels, average='weighted', zero_division=1)\n",
    "    res_recall = recall_score(true_labels, predicted_labels, average='weighted', zero_division=1)\n",
    "    res_f1 = f1_score(true_labels, predicted_labels, average='weighted')\n",
    "    return [res_acc,res_pre,res_recall,res_f1]\n",
    "\n",
    "\n",
    "\n",
    "tmp_next_remain_epochs = g_epoch_count / 2\n",
    "\n",
    "optimizer = optim.Adam(net.parameters(), lr=lr_)\n",
    "\n",
    "for epoch in range(g_epoch_count):\n",
    "    if (g_epoch_count - epoch) < tmp_next_remain_epochs: # To decrease the learning rate by a factor of ten every half of the training epochs\n",
    "        lr_ /= 10\n",
    "    tmp_next_remain_epochs = tmp_next_remain_epochs/2\n",
    "    # if (epoch+1) % 100 == 0: # 每N轮减半\n",
    "    #     lr_ *= 0.5\n",
    "    optimizer = optim.Adam(net.parameters(), lr=lr_)\n",
    "\n",
    "    \n",
    "    epoch_running_loss = 0.0\n",
    "    running_loss = 0.0\n",
    "    epoch_running_cout = 0\n",
    "    for i, (IR_data, distance_data, labels, _) in enumerate(train_dataloader, 0):\n",
    "        IR_data = IR_data.to(mydevice)\n",
    "        distance_data = distance_data.to(mydevice)\n",
    "        labels = labels.to(mydevice)\n",
    "        # 清零梯度\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # 前向传播\n",
    "        outputs = net(IR_data, distance_data)\n",
    "        # 计算损失\n",
    "        loss = criterion(outputs, labels)\n",
    "        # 反向传播\n",
    "        loss.backward()\n",
    "        # 更新权重\n",
    "        optimizer.step()\n",
    "\n",
    "        # 打印统计信息\n",
    "        epoch_running_loss += loss.cpu().item()\n",
    "        running_loss += loss.cpu().item()\n",
    "        epoch_running_cout += 1\n",
    "\n",
    "        if i % 20 == 19:  # 每10个批次打印一次\n",
    "             loss_mean = running_loss / 20\n",
    "             loss_history.append(loss_mean)\n",
    "             # print(f\"[{epoch + 1}, {i + 1}] loss: {loss_mean:.3f} lr:{lr_:3e}\")\n",
    "             running_loss = 0.0\n",
    "\n",
    "    if epoch_running_cout > 0:\n",
    "        epoch_loss_mean = epoch_running_loss / epoch_running_cout\n",
    "        loss_epoch_history.append(epoch_loss_mean)\n",
    "        # print(f\"[{epoch + 1}] loss: {epoch_loss_mean:.3f} lr:{lr_:3e}\")\n",
    "        # if i % 20 == 19:  # 每10个批次打印一次\n",
    "        #     loss_mean = running_loss / 20\n",
    "        #     loss_history.append(loss_mean)\n",
    "        #     print(f\"[{epoch + 1}, {i + 1}] loss: {loss_mean:.3f}\")\n",
    "        #     running_loss = 0.0\n",
    "\n",
    "    # 测试每一轮输出结构\n",
    "    # 保存最新一次\n",
    "    if len(last_modle_file_name)>0 and os.path.exists(last_modle_file_name):\n",
    "        os.remove(last_modle_file_name)\n",
    "\n",
    "    if not os.path.exists(check_points_dir):\n",
    "        os.makedirs(check_points_dir)\n",
    "\n",
    "    last_modle_file_name = f'{check_points_dir}/last_epoch_{epoch}.pth'\n",
    "    torch.save(net.state_dict(), last_modle_file_name)\n",
    "\n",
    "    tmp_epoch_acc,tmp_epoch_pre,tmp_epoch_recall,tmp_epoch_f1 = epoch_test()\n",
    "\n",
    "    if g_save_during > 0 and ((epoch+1) % g_save_during) == 0: # 每n轮保存一次\n",
    "        save_tmp_file_name = f'{check_points_dir}/all/epoch_{epoch}.pth'\n",
    "\n",
    "        if not os.path.exists(f'{check_points_dir}/all/'):\n",
    "            os.makedirs(f'{check_points_dir}/all/')\n",
    "\n",
    "        torch.save(net.state_dict(), save_tmp_file_name)\n",
    "        g_save_model_files.append(save_tmp_file_name)\n",
    "        \n",
    "    print(f\"[{epoch + 1}] loss: {epoch_loss_mean:.3f} f1:{tmp_epoch_f1:3f} acc:{tmp_epoch_acc:3f} pre:{tmp_epoch_pre:3f} recall:{tmp_epoch_recall:3f}\")    \n",
    "\n",
    "    if epoch == 0:\n",
    "        best_recall_score =tmp_epoch_recall\n",
    "        best_acc_score =tmp_epoch_acc\n",
    "        best_pre_score =tmp_epoch_pre\n",
    "        best_f1_score = tmp_epoch_f1\n",
    "        best_recall_modle_file_name = f'{check_points_dir}/best_recall_{epoch}.pth'\n",
    "        best_acc_modle_file_name = f'{check_points_dir}/best_acc_{epoch}.pth'\n",
    "        best_pre_modle_file_name = f'{check_points_dir}/best_pre_{epoch}.pth'\n",
    "        best_f1_modle_file_name = f'{check_points_dir}/best_f1_{epoch}.pth'\n",
    "        torch.save(net.state_dict(), best_recall_modle_file_name)\n",
    "        torch.save(net.state_dict(), best_acc_modle_file_name)\n",
    "        torch.save(net.state_dict(), best_pre_modle_file_name)\n",
    "        torch.save(net.state_dict(), best_f1_modle_file_name)\n",
    "    else:\n",
    "        if (tmp_epoch_recall > best_recall_score):\n",
    "            best_recall_score = tmp_epoch_recall\n",
    "            if len(best_recall_modle_file_name)>0 and os.path.exists(best_recall_modle_file_name):\n",
    "                os.remove(best_recall_modle_file_name)\n",
    "            best_recall_modle_file_name = f'{check_points_dir}/best_recall_{epoch}.pth'\n",
    "            torch.save(net.state_dict(), best_recall_modle_file_name)\n",
    "\n",
    "        if (tmp_epoch_acc > best_acc_score):\n",
    "            best_acc_score = tmp_epoch_acc\n",
    "            if len(best_acc_modle_file_name)>0 and os.path.exists(best_acc_modle_file_name):\n",
    "                os.remove(best_acc_modle_file_name)\n",
    "            best_acc_modle_file_name = f'{check_points_dir}/best_acc_{epoch}.pth'\n",
    "            torch.save(net.state_dict(), best_acc_modle_file_name)\n",
    "\n",
    "        if (tmp_epoch_pre > best_pre_score):\n",
    "            best_pre_score = tmp_epoch_pre\n",
    "            if len(best_pre_modle_file_name)>0 and os.path.exists(best_pre_modle_file_name):\n",
    "                os.remove(best_pre_modle_file_name)\n",
    "            best_pre_modle_file_name = f'{check_points_dir}/best_pre_{epoch}.pth'\n",
    "            torch.save(net.state_dict(), best_pre_modle_file_name)\n",
    "\n",
    "        if (tmp_epoch_f1 > best_f1_score):\n",
    "            best_f1_score = tmp_epoch_f1\n",
    "            if len(best_f1_modle_file_name)>0 and os.path.exists(best_f1_modle_file_name):\n",
    "                os.remove(best_f1_modle_file_name)\n",
    "            best_f1_modle_file_name = f'{check_points_dir}/best_f1_{epoch}.pth'\n",
    "            torch.save(net.state_dict(), best_f1_modle_file_name)\n",
    "\n",
    "        \n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 2000x600 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "os.environ['KMP_DUPLICATE_LIB_OK']='True'\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(20, 6))\n",
    "plt.subplot(121)\n",
    "plt.plot(loss_history)\n",
    "plt.ylim((4.2,8.5))\n",
    "\n",
    "plt.subplot(122)\n",
    "plt.plot(loss_epoch_history)\n",
    "plt.ylim((4.2,8.5))\n",
    "\n",
    "plt.savefig(f'{check_points_dir}/loss_plot.png', dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8926\n",
      "Precision: 0.8947\n",
      "Recall: 0.8926\n",
      "\n",
      "labels are  ['idle', 'sit', 'sit2stand', 'stand', 'stand2sit']\n",
      "Precision per class: [0.91304348 0.93023256 0.88888889 0.88372093 0.81818182]\n",
      "Recall per class: [0.95454545 0.81632653 0.94117647 0.95       0.85714286]\n",
      "\n",
      "混淆矩阵:\n",
      "[[21  0  0  1  0]\n",
      " [ 1 40  0  4  4]\n",
      " [ 0  1 16  0  0]\n",
      " [ 1  1  0 38  0]\n",
      " [ 0  1  2  0 18]]\n",
      "\n",
      "\n",
      "有人判断为无人的异常概率：0.0134\n",
      "无人判断为有人的异常概率：0.0067\n",
      "其他类型误识别成站：0.0336\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score\n",
    "\n",
    "# 测试循环\n",
    "true_labels = []\n",
    "predicted_labels = []\n",
    "filenames = []\n",
    "with torch.no_grad():\n",
    "    for i, (IR_data, distance_data, labels, filename) in enumerate(test_dataloader):\n",
    "        IR_data = IR_data.to(mydevice)\n",
    "        distance_data = distance_data.to(mydevice)\n",
    "\n",
    "        outputs = net(IR_data, distance_data)\n",
    "        outputs = outputs.cpu()\n",
    "\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        _, gt = torch.max(labels.data, 1)\n",
    "        \n",
    "        true_labels.extend(gt.numpy())\n",
    "        predicted_labels.extend(predicted.numpy())\n",
    "        filenames += filename\n",
    "\n",
    "# 计算性能指标\n",
    "accuracy = accuracy_score(true_labels, predicted_labels)\n",
    "precision = precision_score(true_labels, predicted_labels, average='weighted', zero_division=1) # macro\n",
    "recall = recall_score(true_labels, predicted_labels, average='weighted', zero_division=1)\n",
    "\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\\n\")\n",
    "\n",
    "# 计算每个类别的精确度和召回率\n",
    "precision_per_class = precision_score(true_labels, predicted_labels, average=None, zero_division=0)\n",
    "recall_per_class = recall_score(true_labels, predicted_labels, average=None, zero_division=0)\n",
    "\n",
    "# 打印结果\n",
    "print(\"labels are \", ['idle', 'sit', 'sit2stand', 'stand', 'stand2sit'])\n",
    "print(f\"Precision per class: {precision_per_class}\")\n",
    "print(f\"Recall per class: {recall_per_class}\\n\")\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# 计算混淆矩阵\n",
    "cm = confusion_matrix(true_labels, predicted_labels)\n",
    "\n",
    "print(\"混淆矩阵:\")\n",
    "print(cm)\n",
    "\n",
    "print(\"\\n\")\n",
    "\n",
    "tmp_test_count = cm.sum()\n",
    "\n",
    "# 无人判断为有人\n",
    "idle_err_count  = (cm[1][0] + cm[2][0] + cm[3][0] + cm[4][0])\n",
    "print(f\"有人判断为无人的异常概率：{idle_err_count/tmp_test_count:.4f}\")\n",
    "\n",
    "no_idle_err_count  = (cm[0][1] + cm[0][2] + cm[0][3] + cm[0][4])\n",
    "print(f\"无人判断为有人的异常概率：{no_idle_err_count/tmp_test_count:.4f}\")\n",
    "\n",
    " \n",
    "if g_modle_str is 'low':\n",
    "    # 若是低位模型 - 其他类型误识别为站\n",
    "    low_err_count = (cm[0][3] + cm[1][3] + cm[2][3] + cm[4][3])\n",
    "    print(f\"其他类型误识别成站：{ low_err_count/tmp_test_count:.4f}\")\n",
    "\n",
    "if g_modle_str is 'high':\n",
    "    # 若是高位模型 - 其他类型误识别为坐\n",
    "    high_err_count = (cm[0][1] + cm[2][1] + cm[3][1] + cm[4][1])\n",
    "    print(f\"其他类型误识别成坐：{ high_err_count/tmp_test_count:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "被错误分类的样本: ['../data/low-position-sit/newxusheng_低位_坐姿_1719318771', '../data/low-position-sit/newxusheng_低位_坐姿_1719318762', '../data/low-positon-stand2sit/new岑航斌0627_低位_站姿到坐姿_1719467191', '../data/low-position-sit/newxu2_低位_坐姿_1719369655', '../data/low-position-stand/newxu2_低位_站姿_1719368116', '../data/low-position-sit/new邱飞达_低位_坐姿_1719307897', '../data/low-position-sit/new邱飞达_低位_坐姿_1719307874', '../data/low-position-sit/new黄镓辉_低位_坐姿_1719371148', '../data/low-position-sit2stand/newxu2_低位_坐姿到站姿_1719364420', '../data/low-position-stand/new邱飞达0627_低位_站姿_1719466335', '../data/low-positon-stand2sit/new俞聪_低位_站姿到坐姿_1719372194', '../data/low-position-sit/newxu2_低位_坐姿_1719382145', '../data/low-position-nobody/newxusheng_低位_无人_1719306388', '../data/low-position-sit/new黄镓辉_低位_坐姿_1719371143', '../data/low-position-sit/new邱飞达_低位_坐姿_1719369446', '../data/low-positon-stand2sit/newxusheng_低位_站姿到坐姿_1719313418']\n"
     ]
    }
   ],
   "source": [
    "# 找出被错误分类的样本\n",
    "misclassified_samples = [filenames[i] for i in range(len(filenames)) if true_labels[i] != predicted_labels[i]]\n",
    "print(\"被错误分类的样本:\", misclassified_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "被错误分类的样本已复制到 wrongs/20240627-153120\n"
     ]
    }
   ],
   "source": [
    "import shutil\n",
    "import os\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "\n",
    "# 创建目标文件夹\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "target_folder = os.path.join(\"wrongs\", timestamp)\n",
    "os.makedirs(target_folder, exist_ok=True)\n",
    "\n",
    "# 复制被错误分类的样本到目标文件夹\n",
    "for filename in misclassified_samples:\n",
    "    # 假设原始文件位于当前文件夹中\n",
    "    # folder, sample_id = filename.split('_')\n",
    "    source_path = f'{filename}.mp4' # Path('..') / 'data_v2' / folder / f'{sample_id}.mp4'\n",
    "    # target_path = os.path.join(target_folder, os.path.basename(filename))\n",
    "    shutil.copy(source_path, target_folder)\n",
    "\n",
    "print(f\"被错误分类的样本已复制到 {target_folder}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total parameters: 3165\n"
     ]
    }
   ],
   "source": [
    "total_params = sum(p.numel() for p in net.parameters() if p.requires_grad)\n",
    "print(f\"Total parameters: {total_params}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from datetime import datetime\n",
    "# # 获取当前日期和时间\n",
    "# today = datetime.today()\n",
    "# modle_file_name = f'checkpoints/{g_modle_str}/AllData_{today.month}_{today.day}_{g_modle_str}_balanced_{g_modle_extra_str}.pth'\n",
    "# print(f'file_name:{modle_file_name}')\n",
    "# torch.save(net.state_dict(), modle_file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pth_converter import save_params_to_txt\n",
    "\n",
    "# # 使用示例\n",
    "# model_path = 'models\\checkpoints\\low\\AllData_6_11_low_balanced_0d90_3.pth'  # 模型文件路径\n",
    "output_file = f'{last_modle_file_name[:-3]}txt'  # 输出文件路径\n",
    "save_params_to_txt(last_modle_file_name, output_file, g_modle_str, op_declares = False)\n",
    "\n",
    "\n",
    "output_file = f'{best_recall_modle_file_name[:-3]}txt'  # 输出文件路径\n",
    "save_params_to_txt(best_recall_modle_file_name, output_file, g_modle_str, op_declares = False)\n",
    "\n",
    "output_file = f'{best_pre_modle_file_name[:-3]}txt'  # 输出文件路径\n",
    "save_params_to_txt(best_pre_modle_file_name, output_file, g_modle_str, op_declares = False)\n",
    "\n",
    "output_file = f'{best_acc_modle_file_name[:-3]}txt'  # 输出文件路径\n",
    "save_params_to_txt(best_acc_modle_file_name, output_file, g_modle_str, op_declares = False)\n",
    "\n",
    "output_file = f'{best_f1_modle_file_name[:-3]}txt'  # 输出文件路径\n",
    "save_params_to_txt(best_f1_modle_file_name, output_file, g_modle_str, op_declares = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "模型：checkpoints/low/2024_6_27_15_22_53//last_epoch_999.pth\n",
      "Accuracy: 0.8926\n",
      "Precision: 0.8947\n",
      "Recall: 0.8926\n",
      "\n",
      "f1_value: 0.8919\n",
      "\n",
      "labels are  ['idle', 'sit', 'sit2stand', 'stand', 'stand2sit']\n",
      "Precision per class: [0.91304348 0.93023256 0.88888889 0.88372093 0.81818182]\n",
      "Recall per class: [0.95454545 0.81632653 0.94117647 0.95       0.85714286]\n",
      "\n",
      "混淆矩阵:\n",
      "[[21  0  0  1  0]\n",
      " [ 1 40  0  4  4]\n",
      " [ 0  1 16  0  0]\n",
      " [ 1  1  0 38  0]\n",
      " [ 0  1  2  0 18]]\n",
      "\n",
      "\n",
      "total_samples_per_class:[22 49 17 40 21]\n",
      "有人判断为无人的异常概率：0.0157\n",
      "无人判断为有人的异常概率：0.0455\n",
      "其他类型误识别成站：0.0459\n",
      "被错误分类的样本: ['../data/low-position-sit/newxusheng_低位_坐姿_1719318771', '../data/low-position-sit/newxusheng_低位_坐姿_1719318762', '../data/low-positon-stand2sit/new岑航斌0627_低位_站姿到坐姿_1719467191', '../data/low-position-sit/newxu2_低位_坐姿_1719369655', '../data/low-position-stand/newxu2_低位_站姿_1719368116', '../data/low-position-sit/new邱飞达_低位_坐姿_1719307897', '../data/low-position-sit/new邱飞达_低位_坐姿_1719307874', '../data/low-position-sit/new黄镓辉_低位_坐姿_1719371148', '../data/low-position-sit2stand/newxu2_低位_坐姿到站姿_1719364420', '../data/low-position-stand/new邱飞达0627_低位_站姿_1719466335', '../data/low-positon-stand2sit/new俞聪_低位_站姿到坐姿_1719372194', '../data/low-position-sit/newxu2_低位_坐姿_1719382145', '../data/low-position-nobody/newxusheng_低位_无人_1719306388', '../data/low-position-sit/new黄镓辉_低位_坐姿_1719371143', '../data/low-position-sit/new邱飞达_低位_坐姿_1719369446', '../data/low-positon-stand2sit/newxusheng_低位_站姿到坐姿_1719313418']\n",
      "被错误分类的样本已复制到 wrongs/20240627-152253/last_epoch_999\n",
      "模型：checkpoints/low/2024_6_27_15_22_53//best_acc_877.pth\n",
      "Accuracy: 0.8993\n",
      "Precision: 0.9034\n",
      "Recall: 0.8993\n",
      "\n",
      "f1_value: 0.8984\n",
      "\n",
      "labels are  ['idle', 'sit', 'sit2stand', 'stand', 'stand2sit']\n",
      "Precision per class: [0.84       0.95238095 0.88888889 0.88636364 0.9       ]\n",
      "Recall per class: [0.95454545 0.81632653 0.94117647 0.975      0.85714286]\n",
      "\n",
      "混淆矩阵:\n",
      "[[21  0  0  1  0]\n",
      " [ 3 40  0  4  2]\n",
      " [ 0  1 16  0  0]\n",
      " [ 1  0  0 39  0]\n",
      " [ 0  1  2  0 18]]\n",
      "\n",
      "\n",
      "total_samples_per_class:[22 49 17 40 21]\n",
      "有人判断为无人的异常概率：0.0315\n",
      "无人判断为有人的异常概率：0.0455\n",
      "其他类型误识别成站：0.0459\n",
      "被错误分类的样本: ['../data/low-position-sit/newxusheng_低位_坐姿_1719318762', '../data/low-positon-stand2sit/new岑航斌0627_低位_站姿到坐姿_1719467191', '../data/low-position-sit/newxu2_低位_坐姿_1719369655', '../data/low-position-sit/new邱飞达_低位_坐姿_1719307897', '../data/low-position-sit/new邱飞达_低位_坐姿_1719307874', '../data/low-position-sit/new黄镓辉_低位_坐姿_1719371148', '../data/low-position-sit2stand/newxu2_低位_坐姿到站姿_1719364420', '../data/low-position-stand/new邱飞达0627_低位_站姿_1719466335', '../data/low-positon-stand2sit/new俞聪_低位_站姿到坐姿_1719372194', '../data/low-position-sit/newxu2_低位_坐姿_1719382145', '../data/low-position-nobody/newxusheng_低位_无人_1719306388', '../data/low-position-sit/new黄镓辉_低位_坐姿_1719371143', '../data/low-position-sit/new邱飞达_低位_坐姿_1719310486', '../data/low-position-sit/new邱飞达_低位_坐姿_1719369446', '../data/low-positon-stand2sit/newxusheng_低位_站姿到坐姿_1719313418']\n",
      "被错误分类的样本已复制到 wrongs/20240627-152253/best_acc_877\n",
      "模型：checkpoints/low/2024_6_27_15_22_53//best_pre_938.pth\n",
      "Accuracy: 0.8993\n",
      "Precision: 0.9093\n",
      "Recall: 0.8993\n",
      "\n",
      "f1_value: 0.8980\n",
      "\n",
      "labels are  ['idle', 'sit', 'sit2stand', 'stand', 'stand2sit']\n",
      "Precision per class: [0.84       1.         0.89473684 0.88636364 0.82608696]\n",
      "Recall per class: [0.95454545 0.7755102  1.         0.975      0.9047619 ]\n",
      "\n",
      "混淆矩阵:\n",
      "[[21  0  0  1  0]\n",
      " [ 3 38  0  4  4]\n",
      " [ 0  0 17  0  0]\n",
      " [ 1  0  0 39  0]\n",
      " [ 0  0  2  0 19]]\n",
      "\n",
      "\n",
      "total_samples_per_class:[22 49 17 40 21]\n",
      "有人判断为无人的异常概率：0.0315\n",
      "无人判断为有人的异常概率：0.0455\n",
      "其他类型误识别成站：0.0459\n",
      "被错误分类的样本: ['../data/low-position-sit/newxusheng_低位_坐姿_1719318771', '../data/low-position-sit/newxusheng_低位_坐姿_1719318762', '../data/low-positon-stand2sit/new岑航斌0627_低位_站姿到坐姿_1719467191', '../data/low-position-sit/newxu2_低位_坐姿_1719369655', '../data/low-position-sit/new邱飞达_低位_坐姿_1719307897', '../data/low-position-sit/new邱飞达_低位_坐姿_1719307874', '../data/low-position-sit/new黄镓辉_低位_坐姿_1719371148', '../data/low-position-stand/new邱飞达0627_低位_站姿_1719466335', '../data/low-position-sit/newxu2_低位_坐姿_1719382145', '../data/low-position-nobody/newxusheng_低位_无人_1719306388', '../data/low-position-sit/newxu-0627_低位_坐姿_1719451883', '../data/low-position-sit/new黄镓辉_低位_坐姿_1719371143', '../data/low-position-sit/new邱飞达_低位_坐姿_1719310486', '../data/low-position-sit/new邱飞达_低位_坐姿_1719369446', '../data/low-positon-stand2sit/newxusheng_低位_站姿到坐姿_1719313418']\n",
      "被错误分类的样本已复制到 wrongs/20240627-152253/best_pre_938\n",
      "模型：checkpoints/low/2024_6_27_15_22_53//best_recall_877.pth\n",
      "Accuracy: 0.8993\n",
      "Precision: 0.9034\n",
      "Recall: 0.8993\n",
      "\n",
      "f1_value: 0.8984\n",
      "\n",
      "labels are  ['idle', 'sit', 'sit2stand', 'stand', 'stand2sit']\n",
      "Precision per class: [0.84       0.95238095 0.88888889 0.88636364 0.9       ]\n",
      "Recall per class: [0.95454545 0.81632653 0.94117647 0.975      0.85714286]\n",
      "\n",
      "混淆矩阵:\n",
      "[[21  0  0  1  0]\n",
      " [ 3 40  0  4  2]\n",
      " [ 0  1 16  0  0]\n",
      " [ 1  0  0 39  0]\n",
      " [ 0  1  2  0 18]]\n",
      "\n",
      "\n",
      "total_samples_per_class:[22 49 17 40 21]\n",
      "有人判断为无人的异常概率：0.0315\n",
      "无人判断为有人的异常概率：0.0455\n",
      "其他类型误识别成站：0.0459\n",
      "被错误分类的样本: ['../data/low-position-sit/newxusheng_低位_坐姿_1719318762', '../data/low-positon-stand2sit/new岑航斌0627_低位_站姿到坐姿_1719467191', '../data/low-position-sit/newxu2_低位_坐姿_1719369655', '../data/low-position-sit/new邱飞达_低位_坐姿_1719307897', '../data/low-position-sit/new邱飞达_低位_坐姿_1719307874', '../data/low-position-sit/new黄镓辉_低位_坐姿_1719371148', '../data/low-position-sit2stand/newxu2_低位_坐姿到站姿_1719364420', '../data/low-position-stand/new邱飞达0627_低位_站姿_1719466335', '../data/low-positon-stand2sit/new俞聪_低位_站姿到坐姿_1719372194', '../data/low-position-sit/newxu2_低位_坐姿_1719382145', '../data/low-position-nobody/newxusheng_低位_无人_1719306388', '../data/low-position-sit/new黄镓辉_低位_坐姿_1719371143', '../data/low-position-sit/new邱飞达_低位_坐姿_1719310486', '../data/low-position-sit/new邱飞达_低位_坐姿_1719369446', '../data/low-positon-stand2sit/newxusheng_低位_站姿到坐姿_1719313418']\n",
      "被错误分类的样本已复制到 wrongs/20240627-152253/best_recall_877\n",
      "模型：checkpoints/low/2024_6_27_15_22_53//best_f1_956.pth\n",
      "Accuracy: 0.8993\n",
      "Precision: 0.9000\n",
      "Recall: 0.8993\n",
      "\n",
      "f1_value: 0.8989\n",
      "\n",
      "labels are  ['idle', 'sit', 'sit2stand', 'stand', 'stand2sit']\n",
      "Precision per class: [0.91304348 0.91111111 0.94117647 0.88095238 0.86363636]\n",
      "Recall per class: [0.95454545 0.83673469 0.94117647 0.925      0.9047619 ]\n",
      "\n",
      "混淆矩阵:\n",
      "[[21  0  0  1  0]\n",
      " [ 1 41  0  4  3]\n",
      " [ 0  1 16  0  0]\n",
      " [ 1  2  0 37  0]\n",
      " [ 0  1  1  0 19]]\n",
      "\n",
      "\n",
      "total_samples_per_class:[22 49 17 40 21]\n",
      "有人判断为无人的异常概率：0.0157\n",
      "无人判断为有人的异常概率：0.0455\n",
      "其他类型误识别成站：0.0459\n",
      "被错误分类的样本: ['../data/low-position-sit/newxusheng_低位_坐姿_1719318762', '../data/low-positon-stand2sit/new岑航斌0627_低位_站姿到坐姿_1719467191', '../data/low-position-sit/newxu2_低位_坐姿_1719369655', '../data/low-position-stand/newxu2_低位_站姿_1719368116', '../data/low-position-sit/new邱飞达_低位_坐姿_1719307897', '../data/low-position-sit/new邱飞达_低位_坐姿_1719307874', '../data/low-position-sit/new黄镓辉_低位_坐姿_1719371148', '../data/low-position-sit2stand/newxu2_低位_坐姿到站姿_1719364420', '../data/low-position-stand/new邱飞达0627_低位_站姿_1719466335', '../data/low-positon-stand2sit/new俞聪_低位_站姿到坐姿_1719372194', '../data/low-position-sit/newxu2_低位_坐姿_1719382145', '../data/low-position-nobody/newxusheng_低位_无人_1719306388', '../data/low-position-sit/new黄镓辉_低位_坐姿_1719371143', '../data/low-position-sit/new邱飞达_低位_坐姿_1719369446', '../data/low-position-stand/new黄镓辉_低位_站姿_1719371098']\n",
      "被错误分类的样本已复制到 wrongs/20240627-152253/best_f1_956\n"
     ]
    }
   ],
   "source": [
    "# 衡量所有导出模型\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import shutil\n",
    "import os\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "\n",
    "test_result_dict = {}\n",
    "\n",
    "def model_test(_model_file_name:str,_extra_mod:bool = False):\n",
    "    print(f\"模型：{_model_file_name}\")\n",
    "    _model_base_name =os.path.splitext(os.path.basename(_model_file_name))[0]\n",
    "    # 创建类\n",
    "    eval_net = MyMLP().to(mydevice)\n",
    "    # 加载模型\n",
    "    eval_net.load_state_dict(torch.load(_model_file_name))\n",
    "    # eval 模式\n",
    "    eval_net.eval()\n",
    "\n",
    "\n",
    "    # 测试循环\n",
    "    true_labels = []\n",
    "    predicted_labels = []\n",
    "    filenames = []\n",
    "    with torch.no_grad():\n",
    "        for i, (IR_data, distance_data, labels, filename) in enumerate(test_dataloader):\n",
    "            IR_data = IR_data.to(mydevice)\n",
    "            distance_data = distance_data.to(mydevice)\n",
    "\n",
    "            outputs = eval_net(IR_data, distance_data)\n",
    "            outputs = outputs.cpu()\n",
    "\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            _, gt = torch.max(labels.data, 1)\n",
    "            \n",
    "            true_labels.extend(gt.numpy())\n",
    "            predicted_labels.extend(predicted.numpy())\n",
    "            filenames += filename\n",
    "\n",
    "    # 计算性能指标\n",
    "    accuracy = accuracy_score(true_labels, predicted_labels)\n",
    "    precision = precision_score(true_labels, predicted_labels, average='weighted', zero_division=1) # macro\n",
    "    recall = recall_score(true_labels, predicted_labels, average='weighted', zero_division=1)\n",
    "    f1_value = f1_score(true_labels, predicted_labels, average='weighted')\n",
    "\n",
    "\n",
    "    print(f\"Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"Precision: {precision:.4f}\")\n",
    "    print(f\"Recall: {recall:.4f}\\n\")\n",
    "    print(f\"f1_value: {f1_value:.4f}\\n\")\n",
    "\n",
    "    # 计算每个类别的精确度和召回率\n",
    "    precision_per_class = precision_score(true_labels, predicted_labels, average=None, zero_division=0)\n",
    "    recall_per_class = recall_score(true_labels, predicted_labels, average=None, zero_division=0)\n",
    "\n",
    "    # 打印结果\n",
    "    print(\"labels are \", ['idle', 'sit', 'sit2stand', 'stand', 'stand2sit'])\n",
    "    print(f\"Precision per class: {precision_per_class}\")\n",
    "    print(f\"Recall per class: {recall_per_class}\\n\")\n",
    "\n",
    "    # 计算混淆矩阵\n",
    "    cm = confusion_matrix(true_labels, predicted_labels)\n",
    "\n",
    "    print(\"混淆矩阵:\")\n",
    "    print(cm)\n",
    "    \n",
    "\n",
    "    print(\"\\n\")\n",
    "\n",
    "    tmp_test_count = cm.sum()\n",
    "    # 总测试样本数（每一类的测试总数）\n",
    "    total_samples_per_class = np.sum(cm, axis=1)  # 行求和，每一行对应一个类别的总数 \n",
    "    print(f'total_samples_per_class:{total_samples_per_class}')\n",
    "\n",
    "    # 有人判断为无人\n",
    "    idle_err_count  = cm[1][0] + cm[2][0] + cm[3][0] + cm[4][0]\n",
    "    print(f\"有人判断为无人的异常概率：{idle_err_count/(tmp_test_count - total_samples_per_class[0]):.4f}\")\n",
    "    # 无人判断为有人\n",
    "    no_idle_err_count  = cm[0][1] + cm[0][2] + cm[0][3] + cm[0][4]\n",
    "    print(f\"无人判断为有人的异常概率：{no_idle_err_count/total_samples_per_class[0]:.4f}\")\n",
    "\n",
    "    test_each_result_dict = {\"Accuracy\":accuracy,\"Precision\":precision,\"Recall\":recall,\"F1\":f1_value}\n",
    "    test_each_result_dict[\"有人判断为无人\"] = idle_err_count/tmp_test_count\n",
    "    test_each_result_dict[\"无人判断为有人\"] = no_idle_err_count/tmp_test_count\n",
    "    if g_modle_str is 'low':\n",
    "        # 若是低位模型 - 其他类型误识别为站\n",
    "        low_err_count = (cm[0][3] + cm[1][3] + cm[2][3] + cm[4][3])\n",
    "        print(f\"其他类型误识别成站：{ low_err_count/(tmp_test_count - total_samples_per_class[3]):.4f}\")\n",
    "        test_each_result_dict[\"其他类型误识别成站\"] = low_err_count/tmp_test_count\n",
    "\n",
    "    if g_modle_str is 'high':\n",
    "        # 若是高位模型 - 其他类型误识别为坐\n",
    "        high_err_count = (cm[0][1] + cm[2][1] + cm[3][1] + cm[4][1])\n",
    "        print(f\"其他类型误识别成坐：{ high_err_count/(tmp_test_count - total_samples_per_class[1]):.4f}\")\n",
    "        test_each_result_dict[\"其他类型误识别成坐\"] = high_err_count/tmp_test_count\n",
    "\n",
    "    # 找出被错误分类的样本\n",
    "    misclassified_samples = [filenames[i] for i in range(len(filenames)) if true_labels[i] != predicted_labels[i]]\n",
    "    print(\"被错误分类的样本:\", misclassified_samples)\n",
    "\n",
    "    # 创建目标文件夹\n",
    "    timestamp = lean_today.strftime(\"%Y%m%d-%H%M%S\")\n",
    "    target_folder = os.path.join(\"wrongs\", timestamp, _model_base_name)\n",
    "    os.makedirs(target_folder, exist_ok=True)\n",
    "\n",
    "    # 复制被错误分类的样本到目标文件夹\n",
    "    for filename in misclassified_samples:\n",
    "        # 假设原始文件位于当前文件夹中\n",
    "        # folder, sample_id = filename.split('_')\n",
    "        source_path = f'{filename}.mp4' # Path('..') / 'data_v2' / folder / f'{sample_id}.mp4'\n",
    "        # target_path = os.path.join(target_folder, os.path.basename(filename))\n",
    "        shutil.copy(source_path, target_folder)\n",
    "\n",
    "    print(f\"被错误分类的样本已复制到 {target_folder}\")  \n",
    "\n",
    "    test_each_result_dict[\"wrongs\"] = misclassified_samples\n",
    "    if _extra_mod is True:\n",
    "        test_result_dict[\"extra\"][_model_base_name] = test_each_result_dict\n",
    "    else:\n",
    "        test_result_dict[_model_base_name] = test_each_result_dict\n",
    "    \n",
    "model_test(last_modle_file_name)\n",
    "model_test(best_acc_modle_file_name)\n",
    "model_test(best_pre_modle_file_name)\n",
    "model_test(best_recall_modle_file_name)\n",
    "model_test(best_f1_modle_file_name)\n",
    "\n",
    "test_result_dict[\"extra\"] = {}\n",
    "\n",
    "# 使用 for 循环遍历列表\n",
    "for save_item in g_save_model_files:\n",
    "    model_test(save_item,True)\n",
    "\n",
    "test_result_dict[\"version\"] = learn_model_version\n",
    "test_result_dict[\"date\"] = lean_today.isoformat()\n",
    "\n",
    "import json\n",
    "json_str = json.dumps(test_result_dict,indent=4, ensure_ascii=False)\n",
    "\n",
    "with open(f'{check_points_dir}/info.json', 'w') as file:\n",
    "    file.write(json_str+ '\\n')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
