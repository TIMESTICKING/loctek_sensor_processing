{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from preprocess import make_dataset, scale_IR\n",
    "import torch.optim as optim\n",
    "from model import *\n",
    "from torch.utils.data import Dataset, DataLoader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, distance_dataset, IR_dataset, ground_truth, filename_dataset):\n",
    "        self.IR_dataset = IR_dataset\n",
    "        self.distance_dataset = distance_dataset\n",
    "        self.ground_truth = ground_truth\n",
    "        self.filename_dataset = filename_dataset\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.IR_dataset.shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        IR_data = self.IR_dataset[idx]\n",
    "        distance_data = self.distance_dataset[idx]\n",
    "        label = self.ground_truth[idx]\n",
    "        # 将标签转换为one-hot编码\n",
    "        label_one_hot = torch.zeros(LABEL_NUM)\n",
    "        label_one_hot[label] = 1\n",
    "        return IR_data, distance_data, label_one_hot, self.filename_dataset[idx]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "ParserError",
     "evalue": "Error tokenizing data. C error: Expected 1 fields in line 2, saw 33\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mParserError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m trainset, testset \u001b[38;5;241m=\u001b[39m make_dataset()\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# 创建数据集和数据加载器\u001b[39;00m\n\u001b[0;32m      4\u001b[0m train_dataset \u001b[38;5;241m=\u001b[39m CustomDataset(\u001b[38;5;241m*\u001b[39mtrainset)\n",
      "File \u001b[1;32mf:\\WorkSpace\\Lege_codes\\loctek_sensor_processing-main\\loctek_sensor\\models\\preprocess.py:247\u001b[0m, in \u001b[0;36mmake_dataset\u001b[1;34m()\u001b[0m\n\u001b[0;32m    246\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmake_dataset\u001b[39m():\n\u001b[1;32m--> 247\u001b[0m     datasets \u001b[38;5;241m=\u001b[39m load_preprocess(data_dir\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m../data_v2\u001b[39m\u001b[38;5;124m'\u001b[39m, pre_keywords\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhigh-posi*\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m    248\u001b[0m     \u001b[38;5;66;03m# 准备训练集\u001b[39;00m\n\u001b[0;32m    249\u001b[0m     train_dataset, test_dataset \u001b[38;5;241m=\u001b[39m prepare_datasets(datasets, \u001b[38;5;241m0.8\u001b[39m, \u001b[38;5;241m14\u001b[39m, \u001b[38;5;241m9\u001b[39m)\n",
      "File \u001b[1;32mf:\\WorkSpace\\Lege_codes\\loctek_sensor_processing-main\\loctek_sensor\\models\\preprocess.py:126\u001b[0m, in \u001b[0;36mload_preprocess\u001b[1;34m(data_dir, pre_keywords)\u001b[0m\n\u001b[0;32m    123\u001b[0m mat_file \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(folder_path, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfilename\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.mat\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    125\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(csv_file):\n\u001b[1;32m--> 126\u001b[0m     distance_dataset\u001b[38;5;241m.\u001b[39mappend(distance_preprocess(pd\u001b[38;5;241m.\u001b[39mread_csv(csv_file, header\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mfloat\u001b[39m)\u001b[38;5;241m.\u001b[39mvalues))\n\u001b[0;32m    127\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(mat_file):\n\u001b[0;32m    128\u001b[0m     IR_dataset\u001b[38;5;241m.\u001b[39mappend(scipy\u001b[38;5;241m.\u001b[39mio\u001b[38;5;241m.\u001b[39mloadmat(mat_file)[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mIR_video\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "File \u001b[1;32md:\\Environments\\Anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:948\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[0;32m    935\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m    936\u001b[0m     dialect,\n\u001b[0;32m    937\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    944\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[0;32m    945\u001b[0m )\n\u001b[0;32m    946\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m--> 948\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _read(filepath_or_buffer, kwds)\n",
      "File \u001b[1;32md:\\Environments\\Anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:617\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    614\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n\u001b[0;32m    616\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m parser:\n\u001b[1;32m--> 617\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\u001b[38;5;241m.\u001b[39mread(nrows)\n",
      "File \u001b[1;32md:\\Environments\\Anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1748\u001b[0m, in \u001b[0;36mTextFileReader.read\u001b[1;34m(self, nrows)\u001b[0m\n\u001b[0;32m   1741\u001b[0m nrows \u001b[38;5;241m=\u001b[39m validate_integer(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnrows\u001b[39m\u001b[38;5;124m\"\u001b[39m, nrows)\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1743\u001b[0m     \u001b[38;5;66;03m# error: \"ParserBase\" has no attribute \"read\"\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m     (\n\u001b[0;32m   1745\u001b[0m         index,\n\u001b[0;32m   1746\u001b[0m         columns,\n\u001b[0;32m   1747\u001b[0m         col_dict,\n\u001b[1;32m-> 1748\u001b[0m     ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine\u001b[38;5;241m.\u001b[39mread(  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[0;32m   1749\u001b[0m         nrows\n\u001b[0;32m   1750\u001b[0m     )\n\u001b[0;32m   1751\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[0;32m   1752\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[1;32md:\\Environments\\Anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\c_parser_wrapper.py:234\u001b[0m, in \u001b[0;36mCParserWrapper.read\u001b[1;34m(self, nrows)\u001b[0m\n\u001b[0;32m    232\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    233\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlow_memory:\n\u001b[1;32m--> 234\u001b[0m         chunks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reader\u001b[38;5;241m.\u001b[39mread_low_memory(nrows)\n\u001b[0;32m    235\u001b[0m         \u001b[38;5;66;03m# destructive to chunks\u001b[39;00m\n\u001b[0;32m    236\u001b[0m         data \u001b[38;5;241m=\u001b[39m _concatenate_chunks(chunks)\n",
      "File \u001b[1;32mparsers.pyx:843\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader.read_low_memory\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mparsers.pyx:904\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._read_rows\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mparsers.pyx:879\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._tokenize_rows\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mparsers.pyx:890\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._check_tokenize_status\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mparsers.pyx:2058\u001b[0m, in \u001b[0;36mpandas._libs.parsers.raise_parser_error\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mParserError\u001b[0m: Error tokenizing data. C error: Expected 1 fields in line 2, saw 33\n"
     ]
    }
   ],
   "source": [
    "trainset, testset = make_dataset()\n",
    "\n",
    "# 创建数据集和数据加载器\n",
    "train_dataset = CustomDataset(*trainset)\n",
    "test_dataset = CustomDataset(*testset)\n",
    "\n",
    "# 创建数据加载器\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=BATCH, shuffle=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=BATCH, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = trainset[2]\n",
    "# 计算每个类别出现的次数\n",
    "num_samples = len(labels)\n",
    "num_classes = max(labels) + 1  # 假设类别标签从0开始且连续\n",
    "class_counts = [labels.count(i) for i in range(num_classes)]\n",
    "\n",
    "# 计算每个类别的权重，使用类别频率的倒数\n",
    "weights = [num_samples / class_counts[i] if class_counts[i] > 0 else 0 for i in range(num_classes)]\n",
    "\n",
    "# 转换为Tensor\n",
    "weights_tensor = torch.tensor(weights).to(mydevice)\n",
    "print(weights_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 实例化网络\n",
    "net = MyMLP().to(mydevice)\n",
    "# net = MyCNN().to(mydevice)\n",
    "\n",
    "# 定义损失函数和优化器\n",
    "criterion = nn.CrossEntropyLoss(weight=weights_tensor)\n",
    "optimizer = optim.Adam(net.parameters(), lr=0.001)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_history = []\n",
    "# 训练循环示例\n",
    "for epoch in range(30):\n",
    "    running_loss = 0.0\n",
    "    for i, (IR_data, distance_data, labels, _) in enumerate(train_dataloader, 0):\n",
    "        IR_data = IR_data.to(mydevice)\n",
    "        distance_data = distance_data.to(mydevice)\n",
    "        labels = labels.to(mydevice)\n",
    "        # 清零梯度\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # 前向传播\n",
    "        outputs = net(IR_data, distance_data)\n",
    "        # 计算损失\n",
    "        loss = criterion(outputs, labels)\n",
    "        # 反向传播\n",
    "        loss.backward()\n",
    "        # 更新权重\n",
    "        optimizer.step()\n",
    "\n",
    "        # 打印统计信息\n",
    "        running_loss += loss.cpu().item()\n",
    "        if i % 20 == 19:  # 每10个批次打印一次\n",
    "            loss_mean = running_loss / 20\n",
    "            loss_history.append(loss_mean)\n",
    "            print(f\"[{epoch + 1}, {i + 1}] loss: {loss_mean:.3f}\")\n",
    "            running_loss = 0.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(loss_history)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score\n",
    "\n",
    "# 测试循环\n",
    "true_labels = []\n",
    "predicted_labels = []\n",
    "filenames = []\n",
    "with torch.no_grad():\n",
    "    for i, (IR_data, distance_data, labels, filename) in enumerate(test_dataloader):\n",
    "        IR_data = IR_data.to(mydevice)\n",
    "        distance_data = distance_data.to(mydevice)\n",
    "\n",
    "        outputs = net(IR_data, distance_data)\n",
    "        outputs = outputs.cpu()\n",
    "\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        _, gt = torch.max(labels.data, 1)\n",
    "        \n",
    "        true_labels.extend(gt.numpy())\n",
    "        predicted_labels.extend(predicted.numpy())\n",
    "        filenames += filename\n",
    "\n",
    "# 计算性能指标\n",
    "accuracy = accuracy_score(true_labels, predicted_labels)\n",
    "precision = precision_score(true_labels, predicted_labels, average='weighted', zero_division=1) # macro\n",
    "recall = recall_score(true_labels, predicted_labels, average='weighted', zero_division=1)\n",
    "\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\\n\")\n",
    "\n",
    "# 计算每个类别的精确度和召回率\n",
    "precision_per_class = precision_score(true_labels, predicted_labels, average=None, zero_division=0)\n",
    "recall_per_class = recall_score(true_labels, predicted_labels, average=None, zero_division=0)\n",
    "\n",
    "# 打印结果\n",
    "print(\"labels are \", ['idle', 'sit', 'sit2stand', 'stand', 'stand2sit'])\n",
    "print(f\"Precision per class: {precision_per_class}\")\n",
    "print(f\"Recall per class: {recall_per_class}\\n\")\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# 计算混淆矩阵\n",
    "cm = confusion_matrix(true_labels, predicted_labels)\n",
    "\n",
    "print(\"混淆矩阵:\")\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 找出被错误分类的样本\n",
    "misclassified_samples = [filenames[i] for i in range(len(filenames)) if true_labels[i] != predicted_labels[i]]\n",
    "print(\"被错误分类的样本:\", misclassified_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "import os\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "\n",
    "# 创建目标文件夹\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "target_folder = os.path.join(\"wrongs\", timestamp)\n",
    "os.makedirs(target_folder, exist_ok=True)\n",
    "\n",
    "# 复制被错误分类的样本到目标文件夹\n",
    "for filename in misclassified_samples:\n",
    "    # 假设原始文件位于当前文件夹中\n",
    "    # folder, sample_id = filename.split('_')\n",
    "    source_path = f'{filename}.mp4' # Path('..') / 'data_v2' / folder / f'{sample_id}.mp4'\n",
    "    # target_path = os.path.join(target_folder, os.path.basename(filename))\n",
    "    shutil.copy(source_path, target_folder)\n",
    "\n",
    "print(f\"被错误分类的样本已复制到 {target_folder}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_params = sum(p.numel() for p in net.parameters() if p.requires_grad)\n",
    "print(f\"Total parameters: {total_params}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(net.state_dict(), 'checkpoints_v2/high/AllData_v2_balanced_0d90.pth')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
