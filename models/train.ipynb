{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from preprocess import make_dataset, scale_IR\n",
    "import torch.optim as optim\n",
    "from model import *\n",
    "from torch.utils.data import Dataset, DataLoader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, distance_dataset, IR_dataset, ground_truth, filename_dataset):\n",
    "        self.IR_dataset = IR_dataset\n",
    "        self.distance_dataset = distance_dataset\n",
    "        self.ground_truth = ground_truth\n",
    "        self.filename_dataset = filename_dataset\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.IR_dataset.shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        IR_data = self.IR_dataset[idx]\n",
    "        distance_data = self.distance_dataset[idx]\n",
    "        label = self.ground_truth[idx]\n",
    "        # 将标签转换为one-hot编码\n",
    "        label_one_hot = torch.zeros(LABEL_NUM)\n",
    "        label_one_hot[label] = 1\n",
    "        return IR_data, distance_data, label_one_hot, self.filename_dataset[idx]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "distance length is 0\n",
      "IR length is 0\n",
      "gt length is 0\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "need at least one array to stack",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m trainset, testset \u001b[38;5;241m=\u001b[39m make_dataset()\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# 创建数据集和数据加载器\u001b[39;00m\n\u001b[0;32m      4\u001b[0m train_dataset \u001b[38;5;241m=\u001b[39m CustomDataset(\u001b[38;5;241m*\u001b[39mtrainset)\n",
      "File \u001b[1;32mf:\\WorkSpace\\Lege_codes\\loctek_sensor_processing-main\\loctek_sensor\\models\\preprocess.py:249\u001b[0m, in \u001b[0;36mmake_dataset\u001b[1;34m()\u001b[0m\n\u001b[0;32m    247\u001b[0m datasets \u001b[38;5;241m=\u001b[39m load_preprocess(data_dir\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m../data_v2\u001b[39m\u001b[38;5;124m'\u001b[39m, pre_keywords\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhigh-posi*\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m    248\u001b[0m \u001b[38;5;66;03m# 准备训练集\u001b[39;00m\n\u001b[1;32m--> 249\u001b[0m train_dataset, test_dataset \u001b[38;5;241m=\u001b[39m prepare_datasets(datasets, \u001b[38;5;241m0.8\u001b[39m, \u001b[38;5;241m14\u001b[39m, \u001b[38;5;241m9\u001b[39m)\n\u001b[0;32m    251\u001b[0m \u001b[38;5;66;03m# 打印结果以验证\u001b[39;00m\n\u001b[0;32m    252\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDistance train dataset: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_dataset[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mf:\\WorkSpace\\Lege_codes\\loctek_sensor_processing-main\\loctek_sensor\\models\\preprocess.py:207\u001b[0m, in \u001b[0;36mprepare_datasets\u001b[1;34m(datasets, ratio, num_distance_frames, num_IR_frames)\u001b[0m\n\u001b[0;32m    203\u001b[0m         sampled_train_gt\u001b[38;5;241m.\u001b[39mappend(gt_re)\n\u001b[0;32m    206\u001b[0m \u001b[38;5;66;03m# data scale\u001b[39;00m\n\u001b[1;32m--> 207\u001b[0m train_distance_tensor \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(np\u001b[38;5;241m.\u001b[39mstack(sampled_distance_train_dataset, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m), dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32)\n\u001b[0;32m    208\u001b[0m train_IR_tensor, max_IR, min_IR \u001b[38;5;241m=\u001b[39m scale_IR(np\u001b[38;5;241m.\u001b[39mstack(sampled_IR_train_dataset, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m))\n\u001b[0;32m    210\u001b[0m train_dataset \u001b[38;5;241m=\u001b[39m (train_distance_tensor, train_IR_tensor, sampled_train_gt, [\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mlen\u001b[39m(sampled_train_gt))\n",
      "File \u001b[1;32md:\\Environments\\Anaconda3\\Lib\\site-packages\\numpy\\core\\shape_base.py:445\u001b[0m, in \u001b[0;36mstack\u001b[1;34m(arrays, axis, out, dtype, casting)\u001b[0m\n\u001b[0;32m    443\u001b[0m arrays \u001b[38;5;241m=\u001b[39m [asanyarray(arr) \u001b[38;5;28;01mfor\u001b[39;00m arr \u001b[38;5;129;01min\u001b[39;00m arrays]\n\u001b[0;32m    444\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m arrays:\n\u001b[1;32m--> 445\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mneed at least one array to stack\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m    447\u001b[0m shapes \u001b[38;5;241m=\u001b[39m {arr\u001b[38;5;241m.\u001b[39mshape \u001b[38;5;28;01mfor\u001b[39;00m arr \u001b[38;5;129;01min\u001b[39;00m arrays}\n\u001b[0;32m    448\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(shapes) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m1\u001b[39m:\n",
      "\u001b[1;31mValueError\u001b[0m: need at least one array to stack"
     ]
    }
   ],
   "source": [
    "trainset, testset = make_dataset()\n",
    "\n",
    "# 创建数据集和数据加载器\n",
    "train_dataset = CustomDataset(*trainset)\n",
    "test_dataset = CustomDataset(*testset)\n",
    "\n",
    "# 创建数据加载器\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=BATCH, shuffle=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=BATCH, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = trainset[2]\n",
    "# 计算每个类别出现的次数\n",
    "num_samples = len(labels)\n",
    "num_classes = max(labels) + 1  # 假设类别标签从0开始且连续\n",
    "class_counts = [labels.count(i) for i in range(num_classes)]\n",
    "\n",
    "# 计算每个类别的权重，使用类别频率的倒数\n",
    "weights = [num_samples / class_counts[i] if class_counts[i] > 0 else 0 for i in range(num_classes)]\n",
    "\n",
    "# 转换为Tensor\n",
    "weights_tensor = torch.tensor(weights).to(mydevice)\n",
    "print(weights_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 实例化网络\n",
    "net = MyMLP().to(mydevice)\n",
    "# net = MyCNN().to(mydevice)\n",
    "\n",
    "# 定义损失函数和优化器\n",
    "criterion = nn.CrossEntropyLoss(weight=weights_tensor)\n",
    "optimizer = optim.Adam(net.parameters(), lr=0.001)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_history = []\n",
    "# 训练循环示例\n",
    "for epoch in range(30):\n",
    "    running_loss = 0.0\n",
    "    for i, (IR_data, distance_data, labels, _) in enumerate(train_dataloader, 0):\n",
    "        IR_data = IR_data.to(mydevice)\n",
    "        distance_data = distance_data.to(mydevice)\n",
    "        labels = labels.to(mydevice)\n",
    "        # 清零梯度\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # 前向传播\n",
    "        outputs = net(IR_data, distance_data)\n",
    "        # 计算损失\n",
    "        loss = criterion(outputs, labels)\n",
    "        # 反向传播\n",
    "        loss.backward()\n",
    "        # 更新权重\n",
    "        optimizer.step()\n",
    "\n",
    "        # 打印统计信息\n",
    "        running_loss += loss.cpu().item()\n",
    "        if i % 20 == 19:  # 每10个批次打印一次\n",
    "            loss_mean = running_loss / 20\n",
    "            loss_history.append(loss_mean)\n",
    "            print(f\"[{epoch + 1}, {i + 1}] loss: {loss_mean:.3f}\")\n",
    "            running_loss = 0.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(loss_history)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score\n",
    "\n",
    "# 测试循环\n",
    "true_labels = []\n",
    "predicted_labels = []\n",
    "filenames = []\n",
    "with torch.no_grad():\n",
    "    for i, (IR_data, distance_data, labels, filename) in enumerate(test_dataloader):\n",
    "        IR_data = IR_data.to(mydevice)\n",
    "        distance_data = distance_data.to(mydevice)\n",
    "\n",
    "        outputs = net(IR_data, distance_data)\n",
    "        outputs = outputs.cpu()\n",
    "\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        _, gt = torch.max(labels.data, 1)\n",
    "        \n",
    "        true_labels.extend(gt.numpy())\n",
    "        predicted_labels.extend(predicted.numpy())\n",
    "        filenames += filename\n",
    "\n",
    "# 计算性能指标\n",
    "accuracy = accuracy_score(true_labels, predicted_labels)\n",
    "precision = precision_score(true_labels, predicted_labels, average='weighted', zero_division=1) # macro\n",
    "recall = recall_score(true_labels, predicted_labels, average='weighted', zero_division=1)\n",
    "\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\\n\")\n",
    "\n",
    "# 计算每个类别的精确度和召回率\n",
    "precision_per_class = precision_score(true_labels, predicted_labels, average=None, zero_division=0)\n",
    "recall_per_class = recall_score(true_labels, predicted_labels, average=None, zero_division=0)\n",
    "\n",
    "# 打印结果\n",
    "print(\"labels are \", ['idle', 'sit', 'sit2stand', 'stand', 'stand2sit'])\n",
    "print(f\"Precision per class: {precision_per_class}\")\n",
    "print(f\"Recall per class: {recall_per_class}\\n\")\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# 计算混淆矩阵\n",
    "cm = confusion_matrix(true_labels, predicted_labels)\n",
    "\n",
    "print(\"混淆矩阵:\")\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 找出被错误分类的样本\n",
    "misclassified_samples = [filenames[i] for i in range(len(filenames)) if true_labels[i] != predicted_labels[i]]\n",
    "print(\"被错误分类的样本:\", misclassified_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "import os\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "\n",
    "# 创建目标文件夹\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "target_folder = os.path.join(\"wrongs\", timestamp)\n",
    "os.makedirs(target_folder, exist_ok=True)\n",
    "\n",
    "# 复制被错误分类的样本到目标文件夹\n",
    "for filename in misclassified_samples:\n",
    "    # 假设原始文件位于当前文件夹中\n",
    "    # folder, sample_id = filename.split('_')\n",
    "    source_path = f'{filename}.mp4' # Path('..') / 'data_v2' / folder / f'{sample_id}.mp4'\n",
    "    # target_path = os.path.join(target_folder, os.path.basename(filename))\n",
    "    shutil.copy(source_path, target_folder)\n",
    "\n",
    "print(f\"被错误分类的样本已复制到 {target_folder}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_params = sum(p.numel() for p in net.parameters() if p.requires_grad)\n",
    "print(f\"Total parameters: {total_params}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(net.state_dict(), 'checkpoints_v2/high/AllData_v2_balanced_0d90.pth')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
